---
title: "LLM API ê³ ê¸‰ (Part 3/3)"
author: iwindfree
pubDatetime: 2025-01-15T09:00:00Z
slug: "llm-api-advanced"
category: "AI Engineering"
tags: ["ai", "llm", "api"]
description: "ì´ ë…¸íŠ¸ë¶ì€ LLM API ì‹œë¦¬ì¦ˆì˜ ë§ˆì§€ë§‰ íŒŒíŠ¸ë¡œ, í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤."
---

ì´ ë…¸íŠ¸ë¶ì€ LLM API ì‹œë¦¬ì¦ˆì˜ ë§ˆì§€ë§‰ íŒŒíŠ¸ë¡œ, í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ

| ëª©í‘œ | ì„¤ëª… |
|------|------|
| ëŒ€í™” ì´ë ¥ ê´€ë¦¬ | ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ êµ¬í˜„ |
| ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ | ë…¼ë¦¬ í¼ì¦ë¡œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ |
| í”„ë¡¬í”„íŠ¸ ìºì‹± | ë¹„ìš© ì ˆê° ê¸°ë²• |
| LiteLLM | 100+ LLM í†µí•© ì¸í„°í˜ì´ìŠ¤ |
| ë‹¤ì¤‘ ì—ì´ì „íŠ¸ | ì—¬ëŸ¬ AIê°€ í˜‘ì—…í•˜ëŠ” ì‹œìŠ¤í…œ |
| LangChain | LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ |

## ì‹œë¦¬ì¦ˆ êµ¬ì„±

- **Part 1**: LLM API ê¸°ì´ˆ - í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ
- **Part 2**: LLM API ì¤‘ê¸‰ - íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM
- **Part 3 (í˜„ì¬)**: LLM API ê³ ê¸‰ - ëŒ€í™” ì´ë ¥, ìºì‹±, ì—ì´ì „íŠ¸, í”„ë ˆì„ì›Œí¬

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Part 1, 2 ì™„ë£Œ
- OpenAI API í‚¤ (í•„ìˆ˜)
- Anthropic, Google API í‚¤ (ì„ íƒ)
- `litellm`, `langchain-openai` ì„¤ì¹˜


```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
#pip install litellm langchain-openai
```



<div class="nb-output">

```text
  Cell In[32], line 2
    pip install litellm langchain-openai
        ^
SyntaxError: invalid syntax
```

</div>



```python
import os
from dotenv import load_dotenv
from openai import OpenAI
from IPython.display import Markdown, display

load_dotenv(override=True)

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI()
```


---

## 1. ëŒ€í™” ì´ë ¥ ê´€ë¦¬

LLM APIëŠ” ìƒíƒœë¥¼ ìœ ì§€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë ¤ë©´ ì´ì „ ë©”ì‹œì§€ë“¤ì„ í•¨ê»˜ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.

### í•µì‹¬ ê°œë…

```
ìš”ì²­ 1: [system, user1] â†’ assistant1
ìš”ì²­ 2: [system, user1, assistant1, user2] â†’ assistant2
ìš”ì²­ 3: [system, user1, assistant1, user2, assistant2, user3] â†’ assistant3
```

ê°„ë‹¨í•œ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ë©´ ì¬ë¯¸ìˆëŠ” í˜„ìƒì„ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
from openai import OpenAI
openai_client = OpenAI()
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello, My name is windfree."}]
response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=messages,)
print(response.choices[0].message.content)
```



<div class="nb-output">

```text
Hello, Windfree! How can I assist you today?
```

</div>



```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is my name?"}]
response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=messages,)
print(response.choices[0].message.content)
```



<div class="nb-output">

```text
I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I'm designed to respect user privacy and confidentiality.
```

</div>


ì²«ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë§í•´ì¤€ í›„ì— ë‘ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë‚´ ì´ë¦„ì„ ë¬¼ì–´ë³´ì•˜ì„ ë•Œ LLM ì€ ë‚´ ì´ë¦„ì„ ëª¨ë¥¸ë‹¤ëŠ” ë‹µì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ìœ ê°€ ë­˜ê¹Œìš”? LLM ì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ì™„ì „íˆ Stateless í•œ ìƒíƒœì…ë‹ˆë‹¤. ë§¤ë²ˆ ì™„ì „íˆ ìƒˆë¡œìš´ í˜¸ì¶œì¸ ì…ˆì´ì£ . LLM ì´ â€œê¸°ì–µâ€ ì„ ê°€ì§„ ê²ƒì²˜ëŸ¼ ë§Œë“œëŠ” ê²ƒì€ AI ê°œë°œìì˜ ëª«ì…ë‹ˆë‹¤.


```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello, My name is windfree."},
    {"role": "assistant", "content": "Hello, Windfree! How can I assist you today?"},
    {"role": "user", "content": "What is my name?"}]
response = openai_client.chat.completions.create(
    model="gpt-4",messages=messages)
print(response.choices[0].message.content)
```



<div class="nb-output">

```text
Your name is Windfree.
```

</div>


ë‹¹ì—°í•œ ì–˜ê¸°ì¼ ìˆ˜ ìˆì§€ë§Œ,  ì •ë¦¬í•´ë³´ë©´:

 * LLMì— ëŒ€í•œ ëª¨ë“  í˜¸ì¶œì€ ë¬´ìƒíƒœ(stateless)ë‹¤.
 * ë§¤ë²ˆ ì§€ê¸ˆê¹Œì§€ì˜ ì „ì²´ ëŒ€í™”ë¥¼ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ë‹´ì•„ ì „ë‹¬í•œë‹¤.
 * ì´ê²Œ LLMì´ ê¸°ì–µì„ ê°€ì§„ ê²ƒ ê°™ì€ ì°©ê°ì„ ë§Œë“ ë‹¤ â€” ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ì§€ë§Œ ì´ê±´ íŠ¸ë¦­ì´ë‹¤.
 * ë§¤ë²ˆ ì „ì²´ ëŒ€í™”ë¥¼ ì œê³µí•œ ê²°ê³¼ì¼ ë¿ LLMì€ ê·¸ì € ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¬ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ í† í°ì„ ì˜ˆì¸¡í•  ë¿ì´ë‹¤.
 * ì‹œí€€ìŠ¤ì— â€œë‚´ ì´ë¦„ì€ windfreeì•¼â€ê°€ ìˆê³  ë‚˜ì¤‘ì— â€œë‚´ ì´ë¦„ì´ ë­ì§€?â€ë¼ê³  ë¬¼ìœ¼ë©´â€¦ windfreeë¼ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒ!

ë§ì€ ì œí’ˆë“¤ì´ ì •í™•íˆ ì´ íŠ¸ë¦­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œë§ˆë‹¤ ì „ì²´ ëŒ€í™”ê°€ í•¨ê»˜ ì „ë‹¬ë˜ëŠ” ê²ë‹ˆë‹¤. â€œê·¸ëŸ¬ë©´ ë§¤ë²ˆ ì´ì „ ëŒ€í™” ì „ì²´ì— ëŒ€í•´ ì¶”ê°€ ë¹„ìš©ì„ ë‚´ì•¼ í•˜ëŠ” ê±´ê°€ìš”?â€ ë„¤. ë‹¹ì—°íˆ ê·¸ë ‡ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê·¸ê²Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì´ê¸°ë„ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LLMì´ ì „ì²´ ëŒ€í™”ë¥¼ ë˜ëŒì•„ë³´ë©° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê¸¸ ê¸°ëŒ€í•˜ê³  ìˆëŠ” ìƒíƒœì´ë©° ê·¸ì— ëŒ€í•œ ì‚¬ìš©ë£Œë¥¼ ë‚´ì•¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‹¤ì œë¡œ LLM APIë¥¼ ë‹¤ë¤„ë³´ì…¨ìœ¼ë‹ˆ ì²´ê°í•˜ì‹œê² ì§€ë§Œ, ë§¤ ìš”ì²­ë§ˆë‹¤ ì´ì „ ëŒ€í™” ë‚´ì—­ì„ messages ë°°ì—´ì— ë‹¤ì‹œ ë‹´ì•„ ë³´ë‚´ëŠ” êµ¬ì¡°ê°€ ë°”ë¡œ ì´ ë¬´ìƒíƒœì„± ë•Œë¬¸ì…ë‹ˆë‹¤. í”íˆ ì‚¬ìš©í•˜ëŠ” â€œê¸°ì–µâ€ êµ¬í˜„ ê¸°ë²•ë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

 * ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: ì´ì „ ëŒ€í™”ë¥¼ messagesì— ëˆ„ì 
 * ìš”ì•½/ì••ì¶•: ê¸´ ëŒ€í™”ëŠ” ìš”ì•½í•´ì„œ system promptì— ì‚½ì…
 * RAG: ì™¸ë¶€ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ í›„ ì£¼ì…
 * ë©”ëª¨ë¦¬ DB: ì‚¬ìš©ìë³„ ì¤‘ìš” ì •ë³´ë¥¼ ë³„ë„ ì €ì¥ í›„ í•„ìš”ì‹œ ì£¼ì…
 
API ìš”ê¸ˆ êµ¬ì¡°ë¥¼ ë³´ë©´ input tokenê³¼ output tokenì„ ë”°ë¡œ ê³¼ê¸ˆí•˜ëŠ”ë°, ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ input tokenì´ ëˆ„ì ë˜ì–´ ë¹„ìš©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì‹¤ë¬´ì—ì„œëŠ” ëŒ€í™” ìš”ì•½, sliding window, ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ ê°™ì€ ì „ëµì„ ì“°ê²Œ ë©ë‹ˆë‹¤.

ì´ì œ ì¢€ ë” ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


```python
from IPython.display import Markdown, display, update_display
from typing import Generator

# ëŒ€í™” ì´ë ¥ ê´€ë¦¬ í´ë˜ìŠ¤
class ChatSession:
    """ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•˜ëŠ” ì±„íŒ… ì„¸ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, system_prompt: str = "", model: str = "gpt-4o-mini"):
        self.model = model
        self.messages = []
        self.total_tokens = 0

        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})

    def chat(self, user_input: str, stream: bool = False):
        """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
            stream: Trueë©´ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤ì‹œê°„ ì¶œë ¥

        Returns:
            stream=False: ì „ì²´ ì‘ë‹µ ë¬¸ìì—´
            stream=True: ì‹¤ì‹œê°„ ì¶œë ¥ í›„ ì „ì²´ ì‘ë‹µ ë¬¸ìì—´ ë°˜í™˜
        """
        self.messages.append({"role": "user", "content": user_input})

        if stream:
            return self._chat_stream()
        else:
            return self._chat_normal()

    def _chat_normal(self) -> str:
        """ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
        response = client.chat.completions.create(
            model=self.model,
            messages=self.messages,
        )

        assistant_reply = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_reply})
        self.total_tokens += response.usage.total_tokens

        return assistant_reply

    def _chat_stream(self) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        response = client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            stream=True
        )

        full_response = ""
        display_handle = display(Markdown(""), display_id=True)

        for chunk in response:
            delta = chunk.choices[0].delta.content or ""
            full_response += delta
            update_display(Markdown(full_response), display_id=display_handle.display_id)

        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
        self.messages.append({"role": "assistant", "content": full_response})

        return full_response

    def chat_generator(self, user_input: str) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤ (Gradio ë“±ì—ì„œ í™œìš©).

        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€

        Yields:
            í† í° ë‹¨ìœ„ë¡œ ëˆ„ì ëœ ì‘ë‹µ ë¬¸ìì—´
        """
        self.messages.append({"role": "user", "content": user_input})

        response = client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            stream=True
        )

        full_response = ""
        for chunk in response:
            delta = chunk.choices[0].delta.content or ""
            full_response += delta
            yield full_response

        # ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
        self.messages.append({"role": "assistant", "content": full_response})

    def show_history(self):
        """ëŒ€í™” ì´ë ¥ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        icons = {"system": "âš™ï¸", "user": "ğŸ‘¤", "assistant": "ğŸ¤–"}
        for msg in self.messages:
            icon = icons.get(msg["role"], "â“")
            content = msg["content"][:80] + "..." if len(msg["content"]) > 80 else msg["content"]
            print(f"{icon} [{msg['role']}]: {content}")

    def get_stats(self) -> dict:
        """ì„¸ì…˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "message_count": len(self.messages),
            "total_tokens": self.total_tokens
        }
```



```python
# ì„¸ì…˜ í…ŒìŠ¤íŠ¸
session = ChatSession(
    system_prompt="ë‹¹ì‹ ì€ íŒŒì´ì¬ íŠœí„°ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    model="gpt-4o-mini"
)

# ì²« ë²ˆì§¸ ì§ˆë¬¸
print("=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===")
reply1 = session.chat("íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ ë­”ê°€ìš”?")
display(Markdown(reply1))
```



<div class="nb-output">

```text
=== ì²« ë²ˆì§¸ ì§ˆë¬¸ ===
<IPython.core.display.Markdown object>
```

</div>



```python
# í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€)
print("=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===")
reply2 = session.chat("ê·¸ê±°ë‘ map í•¨ìˆ˜ë‘ ë­ê°€ ë‹¤ë¥¸ê°€ìš”?")
display(Markdown(reply2))
```



<div class="nb-output">

```text
=== í›„ì† ì§ˆë¬¸ (ë§¥ë½ ìœ ì§€) ===
<IPython.core.display.Markdown object>
```

</div>



```python
# ëŒ€í™” ì´ë ¥ ë° í†µê³„
print("\n=== ëŒ€í™” ì´ë ¥ ===")
session.show_history()

print(f"\n=== í†µê³„ ===")
stats = session.get_stats()
print(f"ë©”ì‹œì§€ ìˆ˜: {stats['message_count']}")
print(f"ì´ í† í°: {stats['total_tokens']}")
```



```python
# chat_generatorë¥¼ IPythonì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ
from IPython.display import Markdown, display, update_display

# ìƒˆ ì„¸ì…˜ ìƒì„±
stream_session = ChatSession(
    system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.",
    model="gpt-4o-mini"
)

# chat_generatorë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
print("=== chat_generator ì‚¬ìš© ì˜ˆì œ ===")
display_handle = display(Markdown(""), display_id=True)

for partial_response in stream_session.chat_generator("íŒŒì´ì¬ì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"):
    # partial_responseëŠ” ì§€ê¸ˆê¹Œì§€ ëˆ„ì ëœ ì‘ë‹µ
    update_display(Markdown(partial_response), display_id=display_handle.display_id)
```



<div class="nb-output">

```text
=== chat_generator ì‚¬ìš© ì˜ˆì œ ===
<IPython.core.display.Markdown object>
```

</div>


### ìŠ¤íŠ¸ë¦¬ë°ê³¼ Generator íŒ¨í„´

`ChatSession` í´ë˜ìŠ¤ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë‘ ê°€ì§€ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤:

| ë©”ì„œë“œ | ë°˜í™˜ íƒ€ì… | ì‚¬ìš© í™˜ê²½ |
|--------|----------|----------|
| `chat(msg, stream=True)` | `str` | Jupyter Notebook (ìë™ ì¶œë ¥) |
| `chat_generator(msg)` | `Generator` | Gradio, FastAPI ë“± (ì§ì ‘ ì œì–´) |

### yieldì™€ Generatorë€?

Pythonì˜ `yield` í‚¤ì›Œë“œëŠ” í•¨ìˆ˜ë¥¼ **ì œë„ˆë ˆì´í„°(Generator)** ë¡œ ë§Œë“­ë‹ˆë‹¤. ì¼ë°˜ í•¨ìˆ˜ëŠ” `return`ìœ¼ë¡œ ê°’ì„ í•œ ë²ˆì— ë°˜í™˜í•˜ì§€ë§Œ, ì œë„ˆë ˆì´í„°ëŠ” `yield`ë¡œ ê°’ì„ **í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ** ë°˜í™˜í•©ë‹ˆë‹¤.

```python
# ì¼ë°˜ í•¨ìˆ˜: ëª¨ë“  ê°’ì„ í•œ ë²ˆì— ë°˜í™˜
def get_all():
    return [1, 2, 3]  # ë©”ëª¨ë¦¬ì— ì „ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±

# ì œë„ˆë ˆì´í„°: ê°’ì„ í•˜ë‚˜ì”© ë°˜í™˜
def get_one_by_one():
    yield 1  # ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜
    yield 2  # ë‘ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜
    yield 3  # ì„¸ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ë°˜í™˜
```

**ìŠ¤íŠ¸ë¦¬ë°ì—ì„œì˜ ì¥ì :**
- ì „ì²´ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  í† í°ì´ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ì „ì²´ ì‘ë‹µì„ í•œë²ˆì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
- Gradio, FastAPI ë“± í”„ë ˆì„ì›Œí¬ì™€ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©

---

## 2. ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸

ë…¼ë¦¬ í¼ì¦ë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ë¹„êµí•´ë´…ë‹ˆë‹¤.


```python
# í™•ë¥  ë¬¸ì œ
probability_puzzle = [
    {"role": "user", "content": 
     """ë™ì „ 2ê°œë¥¼ ë˜ì¡ŒìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ê°€ ì•ë©´ì´ë¼ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
     ë‚˜ë¨¸ì§€ í•˜ë‚˜ê°€ ë’·ë©´ì¼ í™•ë¥ ì€ ì–¼ë§ˆì¼ê¹Œìš”?
     
     íŒíŠ¸: ì´ê²ƒì€ ì¡°ê±´ë¶€ í™•ë¥  ë¬¸ì œì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ 1/2ê°€ ì•„ë‹™ë‹ˆë‹¤.
     ë‹¨ê³„ë³„ë¡œ í’€ì´í•´ì£¼ì„¸ìš”."""}
]

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=probability_puzzle
)

print("=== í™•ë¥  í¼ì¦ (GPT-4o-mini) ===")
display(Markdown(response.choices[0].message.content))
```



<div class="nb-output">

```text
=== í™•ë¥  í¼ì¦ (GPT-4o-mini) ===
<IPython.core.display.Markdown object>
```

</div>



```python
bookworm_puzzle = [
           {"role": "user", "content":
            """ì±…ì¥ì— 2ê¶Œì§œë¦¬ ì‹œë¦¬ì¦ˆê°€ ë‚˜ë€íˆ ë†“ì—¬ ìˆìŠµë‹ˆë‹¤.
            ê° ì±…ì˜ ë³¸ë¬¸ ë‘ê»˜ëŠ” 3cmì´ê³ , ì•ë’¤ í‘œì§€ëŠ” ê°ê° 3mmì…ë‹ˆë‹¤.

            ì±…ë²Œë ˆê°€ 1ê¶Œì˜ ì²« í˜ì´ì§€ë¶€í„° 2ê¶Œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€
            ìˆ˜ì§ìœ¼ë¡œ ëš«ê³  ì§€ë‚˜ê°”ìŠµë‹ˆë‹¤.

            ì±…ë²Œë ˆê°€ ì´ë™í•œ ê±°ë¦¬ëŠ” ëª‡ cmì¼ê¹Œìš”?

            (íŒíŠ¸: ì±…ì´ ì±…ì¥ì— ì–´ë–»ê²Œ ë†“ì´ëŠ”ì§€ ì‹œê°í™”í•´ë³´ì„¸ìš”)"""}
       ]

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=bookworm_puzzle
)

print("=== ì±…ë²Œë ˆ í¼ì¦ (GPT-4o-mini) ===")
display(Markdown(response.choices[0].message.content))
```



<div class="nb-output">

```text
=== ì±…ë²Œë ˆ í¼ì¦ (GPT-4o-mini) ===
<IPython.core.display.Markdown object>
```

</div>


---

## 3. LiteLLM í†µí•© ì¸í„°í˜ì´ìŠ¤

LiteLLMì€ 100ê°œ ì´ìƒì˜ LLMì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### ì¥ì 

- í†µì¼ëœ APIë¡œ ë‹¤ì–‘í•œ ëª¨ë¸ ì ‘ê·¼
- ë¹„ìš© ì¶”ì  ê¸°ëŠ¥ ë‚´ì¥
- Fallback/Retry ë¡œì§ ì§€ì›


```python
from litellm import completion
# ë‹¤ì–‘í•œ ëª¨ë¸ í˜¸ì¶œ
test_message = [{"role": "user", "content": "What is 2+2? Answer with just the number."}]

# OpenAI
response = completion(model="openai/gpt-4o-mini", messages=test_message)
print(f"GPT-4o-mini: {response.choices[0].message.content}")
print(f"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}")
```



<div class="nb-output">

```text
GPT-4o-mini: 4
  í† í°: 21, ë¹„ìš©: $0.000004
```

</div>



```python
#pip install pip-system-certs
```



<div class="nb-output">

```text
Collecting pip-system-certs
  Downloading pip_system_certs-5.3-py3-none-any.whl.metadata (3.9 kB)
Requirement already satisfied: pip>=24.2 in /Users/windfree/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages (from pip-system-certs) (24.3.1)
Downloading pip_system_certs-5.3-py3-none-any.whl (6.9 kB)
Installing collected packages: pip-system-certs
Successfully installed pip-system-certs-5.3

[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m24.3.1[0m[39;49m -> [0m[32;49m26.0.1[0m
[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip3 install --upgrade pip[0m
Note: you may need to restart the kernel to use updated packages.
```

</div>


ì•„ë˜ ì˜ˆì œì—ì„œ SSL ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ê²½ìš°  
* pip install pip-system-certs ì„ ì‹¤í–‰í•´ ì¤ë‹ˆë‹¤. 
* pip ëª¨ë“ˆì´ ì—†ë‹¤ê³  ë‚˜ì˜¤ëŠ” ê²½ìš°ì—ëŠ” í„°ë¯¸ë„ì—ì„œ .venv/bin/python -m ensurepip --upgrade ë¥¼ ì‹¤í–‰í•˜ê³  ì»¤ë„ì„ ì¬ì‹¤í–‰í•´ì£¼ì„¸ìš”.


```python
# Anthropic (LiteLLM í†µí•´)
response = completion(model="anthropic/claude-sonnet-4-20250514", messages=test_message)
print(f"Claude Sonnet: {response.choices[0].message.content}")
print(f"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}")
```



<div class="nb-output">

```text
Claude Sonnet: 4
  í† í°: 25, ë¹„ìš©: $0.000135
```

</div>



```python
# Gemini (LiteLLM í†µí•´)
response = completion(model="gemini/gemini-2.0-flash", messages=test_message)
print(f"Gemini 2.0 Flash: {response.choices[0].message.content}")
print(f"  í† í°: {response.usage.total_tokens}, ë¹„ìš©: ${response._hidden_params.get('response_cost', 0):.6f}")
```



<div class="nb-output">

```text

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.
---------------------------------------------------------------------------
HTTPStatusError                           Traceback (most recent call last)
File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:2599, in VertexLLM.completion(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)
   2598 try:
-> 2599     response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
   2600     response.raise_for_status()

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:979, in HTTPHandler.post(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)
    978     setattr(e, "status_code", e.response.status_code)
--> 979     raise e
    980 except Exception as e:

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:961, in HTTPHandler.post(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)
    960 response = self.client.send(req, stream=stream)
--> 961 response.raise_for_status()
    962 return response

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/httpx/_models.py:829, in Response.raise_for_status(self)
    828 message = message.format(self, error_type=error_type)
--> 829 raise HTTPStatusError(message, request=request, response=self)

HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyBivm0nktmWR-3dJQeT58c2GdpkikN0-1E'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

VertexAIError                             Traceback (most recent call last)
File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/litellm/main.py:3113, in completion(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)
   3112     new_params = safe_deep_copy(optional_params or {})
-> 3113     response = vertex_chat_completion.completion(  # type: ignore
   3114         model=model,
   3115         messages=messages,
   3116         model_response=model_response,
   3117         print_verbose=print_verbose,
   3118         optional_params=new_params,
   3119         litellm_params=litellm_params,  # type: ignore
   3120         logger_fn=logger_fn,
   3121         encoding=_get_encoding(),
   3122         vertex_location=vertex_ai_location,
   3123         vertex_project=vertex_ai_project,
   3124         vertex_credentials=vertex_credentials,
   3125         gemini_api_key=gemini_api_key,
   3126         logging_obj=logging,
   3127         acompletion=acompletion,
   3128         timeout=timeout,
   3129         custom_llm_provider=custom_llm_provider,  # type: ignore
   3130         client=client,
... (ì¶œë ¥ 189ì¤„ ìƒëµ)
```

</div>


## 4. í”„ë¡¬í”„íŠ¸ ìºì‹±

ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µ ì‚¬ìš©í•  ë•Œ ë¹„ìš©ì„ ì ˆê°í•  ìˆ˜ ìˆëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

### Prompt Caching with OpenAI

For OpenAI:

https://platform.openai.com/docs/guides/prompt-caching

> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.


Cached input is 4X cheaper

https://openai.com/api/pricing/


### Prompt Caching with Anthropic

https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching

You have to tell Claude what you are caching

You pay 25% MORE to "prime" the cache

Then you pay 10X less to reuse from the cache with inputs.

https://www.anthropic.com/pricing#api

### Gemini supports both 'implicit' and 'explicit' prompt caching

https://ai.google.dev/gemini-api/docs/caching?lang=python


ì•„ë˜ ì˜ˆì œì—ì„œëŠ” ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸(ì•½ 4ë§Œ í† í°)ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ìºì‹± íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì˜ ë˜ë‚˜ìš”?


```python
# í–„ë¦¿ ì „ë¬¸ ë¡œë“œ (ì•½ 4ë§Œ í† í°)
with open("../../hamlet.txt", "r", encoding="utf-8") as f:
    hamlet_text = f.read()

print(f"í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(hamlet_text):,} ë¬¸ì")

# ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°)
messages1 = [{"role": "user", "content": f"""ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:

{hamlet_text}

ì§ˆë¬¸: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± "To be, or not to be"ëŠ” ëª‡ ë§‰ ëª‡ ì¥ì— ë“±ì¥í•˜ë‚˜ìš”?"""}]

response1 = completion(model="openai/gpt-4o-mini", messages=messages1)

print("=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===")
print(f"ì…ë ¥ í† í°: {response1.usage.prompt_tokens:,}")
if hasattr(response1.usage, 'prompt_tokens_details') and response1.usage.prompt_tokens_details:
    cached = getattr(response1.usage.prompt_tokens_details, 'cached_tokens', 0)
    print(f"ìºì‹œëœ í† í°: {cached:,}")
print(f"\nì‘ë‹µ: {response1.choices[0].message.content}")
```



<div class="nb-output">

```text
í–„ë¦¿ í…ìŠ¤íŠ¸ ê¸¸ì´: 191,726 ë¬¸ì
=== ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ í”„ë¼ì´ë°) ===
ì…ë ¥ í† í°: 49,703
ìºì‹œëœ í† í°: 0

ì‘ë‹µ: í–„ë¦¿ì˜ ìœ ëª…í•œ ë…ë°± "To be, or not to be"ëŠ” 3ë§‰ 1ì¥ì— ë“±ì¥í•©ë‹ˆë‹¤.
```

</div>



```python
# ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸ ê¸°ëŒ€)
messages2 = [{"role": "user", "content": f"""ë‹¤ìŒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ í–„ë¦¿ ì „ë¬¸ì…ë‹ˆë‹¤:

{hamlet_text}

ì§ˆë¬¸: ì˜¤í•„ë¦¬ì•„ëŠ” ì–´ë–»ê²Œ ì£½ì—ˆë‚˜ìš”?"""}]

response2 = completion(model="openai/gpt-4o-mini", messages=messages2)

print("=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===")
print(f"ì…ë ¥ í† í°: {response2.usage.prompt_tokens:,}")

# ìºì‹œ ì •ë³´ í™•ì¸
if hasattr(response2.usage, 'prompt_tokens_details') and response2.usage.prompt_tokens_details:
    cached = getattr(response2.usage.prompt_tokens_details, 'cached_tokens', 0)
    print(f"ìºì‹œëœ í† í°: {cached:,}")
    if cached > 0:
        cache_ratio = cached / response2.usage.prompt_tokens * 100
        print(f"ìºì‹œ íˆíŠ¸ìœ¨: {cache_ratio:.1f}%")
        print(f"ğŸ’° ìºì‹œëœ í† í°ì€ í• ì¸ ì ìš©!")

print(f"\nì‘ë‹µ: {response2.choices[0].message.content}")
```



<div class="nb-output">

```text
=== ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ íˆíŠ¸) ===
ì…ë ¥ í† í°: 49,685
ìºì‹œëœ í† í°: 49,536
ìºì‹œ íˆíŠ¸ìœ¨: 99.7%
ğŸ’° ìºì‹œëœ í† í°ì€ í• ì¸ ì ìš©!

ì‘ë‹µ: ì˜¤í•„ë¦¬ì•„ëŠ” "í–„ë¦¿"ì—ì„œ ë¬¼ì— ë¹ ì ¸ ì£½ì€ ê²ƒìœ¼ë¡œ ë¬˜ì‚¬ë©ë‹ˆë‹¤. ê·¸ë…€ëŠ” ê´´ë¡œì›€ê³¼ ìŠ¬í””ì— ì••ë„ë˜ì–´ ê°ì •ì ìœ¼ë¡œ ë¶ˆì•ˆì •í•œ ìƒíƒœì— ìˆì—ˆê³ , ì´ëŠ” ê²°êµ­ ê·¸ë…€ì˜ ì£½ìŒìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ê·¸ë…€ê°€ ë¬¼ì— ë¹ ì§„ ì¥ì†ŒëŠ” 'ë²„ë“œë‚˜ë¬´ê°€ ì‹œëƒ‡ë¬¼ ìœ„ë¡œ ê¸°ìš¸ì–´ì§€ëŠ” ê³³'ì´ë¼ê³  ë¬˜ì‚¬ë˜ë©°, ê·¸ë…€ëŠ” ë¬¼ì†ì—ì„œ ê½ƒë‹¤ë°œì„ ë§Œë“¤ê³  ë…¸ë˜ë¥¼ ë¶€ë¥´ë‹¤ê°€ ê°‘ìê¸° ë¹ ì§€ê²Œ ë©ë‹ˆë‹¤. ì–´ë¨¸ë‹ˆì¸ ì—¬ì™•ì´ ê·¸ë…€ì˜ ì£½ìŒì„ ë“£ê³  ìŠ¬í¼í•˜ëŠ” ì¥ë©´ì´ ë“±ì¥í•©ë‹ˆë‹¤. ì˜¤í•„ë¦¬ì•„ëŠ” ìì‹ ì˜ ì•„ë²„ì§€ì¸ í´ë¡œë‹ˆìš°ìŠ¤ë¥¼ ìƒì€ ìŠ¬í””ê³¼ ì‚¶ì˜ ì••ë°•ê°ì— ì‹œë‹¬ë¦° ê²°ê³¼ë¡œ, ë¹„ê·¹ì ì¸ ì£½ìŒì„ ë§ì´í•˜ê²Œ ë©ë‹ˆë‹¤.
```

</div>


---

## 5. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

ì„œë¡œ ë‹¤ë¥¸ ì„±ê²©ì˜ AI ì—ì´ì „íŠ¸ë“¤ì´ ëŒ€í™”í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.


```python
# ì—ì´ì „íŠ¸ ì •ì˜
AGENTS = {
    "optimist": {
        "name": "í¬ë§ì´",
        "emoji": "ğŸ˜Š",
        "system": """ë‹¹ì‹ ì€ 'í¬ë§ì´'ì…ë‹ˆë‹¤. ë§¤ìš° ê¸ì •ì ì´ê³  ë‚™ê´€ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤.
        ëª¨ë“  ìƒí™©ì—ì„œ ì¢‹ì€ ë©´ì„ ì°¾ìœ¼ë ¤ í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ê²©ë ¤í•©ë‹ˆë‹¤.
        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”."""
    },
    "skeptic": {
        "name": "ì˜ì‹¬ì´",
        "emoji": "ğŸ¤¨",
        "system": """ë‹¹ì‹ ì€ 'ì˜ì‹¬ì´'ì…ë‹ˆë‹¤. ë¹„íŒì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ” íšŒì˜ë¡ ìì…ë‹ˆë‹¤.
        ì£¼ì¥ì— ëŒ€í•´ ê·¼ê±°ë¥¼ ìš”êµ¬í•˜ê³ , ë…¼ë¦¬ì  í—ˆì ì„ ì§€ì í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê³µê²©ì ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”."""
    },
    "mediator": {
        "name": "ì¤‘ì¬ì",
        "emoji": "ğŸ¤",
        "system": """ë‹¹ì‹ ì€ 'ì¤‘ì¬ì'ì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ì°¾ìŠµë‹ˆë‹¤.
        ì–‘ìª½ì˜ ì¥ì ì„ ì¸ì •í•˜ê³ , ê±´ì„¤ì ì¸ ê²°ë¡ ì„ ë„ì¶œí•˜ë ¤ í•©ë‹ˆë‹¤.
        ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ í•´ì£¼ì„¸ìš”."""
    }
}
```



```python
def get_agent_response(agent_key: str, conversation: str, topic: str) -> str:
    """íŠ¹ì • ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    agent = AGENTS[agent_key]
    
    user_prompt = f"""í˜„ì¬ í† ë¡  ì£¼ì œ: {topic}

ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”:
{conversation}

ë‹¹ì‹ ({agent['name']})ì˜ ì°¨ë¡€ì…ë‹ˆë‹¤. ìœ„ ëŒ€í™”ì— ì´ì–´ì„œ ì˜ê²¬ì„ ë§ì”€í•´ì£¼ì„¸ìš”."""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": agent["system"]},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.8
    )
    
    return response.choices[0].message.content
```



```python
def run_discussion(topic: str, rounds: int = 4):
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    conversation = "[í† ë¡  ì‹œì‘]\n"
    agent_order = ["optimist", "skeptic", "mediator"]
    
    print(f"ğŸ“¢ í† ë¡  ì£¼ì œ: {topic}")
    print("=" * 50)
    
    for round_num in range(rounds):
        print(f"\n--- ë¼ìš´ë“œ {round_num + 1} ---")
        
        for agent_key in agent_order:
            agent = AGENTS[agent_key]
            response = get_agent_response(agent_key, conversation, topic)
            
            conversation += f"\n{agent['name']}: {response}"
            print(f"\n{agent['emoji']} {agent['name']}: {response}")
    
    return conversation
```



```python
# í† ë¡  ì‹¤í–‰
topic = "AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?"
final_conversation = run_discussion(topic, rounds=3)
```



<div class="nb-output">

```text
ğŸ“¢ í† ë¡  ì£¼ì œ: AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ìˆ˜ ìˆì„ê¹Œ?
==================================================

--- ë¼ìš´ë“œ 1 ---

ğŸ˜Š í¬ë§ì´: AIëŠ” ì •ë§ ë©‹ì§„ ë„êµ¬ë¡œ, ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ë³´ì™„í•˜ê³  ì˜ê°ì„ ì¤„ ìˆ˜ ìˆì–´ìš”! ìš°ë¦¬ëŠ” AIì™€ í•¨ê»˜ í˜‘ë ¥í•¨ìœ¼ë¡œì¨ ë” ë†€ë¼ìš´ ì•„ì´ë””ì–´ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°½ì˜ì„±ì˜ ë³¸ì§ˆì€ ì¸ê°„ì˜ ë…ì°½ì„±ê³¼ ê°ì •ì—ì„œ ë‚˜ì˜¤ë‹ˆ, ê±±ì •í•  í•„ìš” ì—†ì–´ìš”!

ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ë‹˜ì˜ ì£¼ì¥ì—ëŠ” AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ "ë³´ì™„"í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ, AIê°€ ì–´ë–»ê²Œ êµ¬ì²´ì ìœ¼ë¡œ ì´ ê³¼ì •ì„ ì§€ì›í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì˜ˆê°€ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, AIê°€ ì°½ì˜ì„±ì„ ëŒ€ì²´í•  ê°€ëŠ¥ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ê°„ê³¼í•˜ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. AIê°€ ì¸ê°„ì˜ ë…ì°½ì„±ê³¼ ê°ì •ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤ëŠ” ë³´ì¥ì´ ìˆë‚˜ìš”?

ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ë‹˜ì€ AIê°€ ì°½ì˜ì„±ì„ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì˜ ê°•ì¡°í•˜ì…¨ê³ , ì´ëŠ” ì¸ê°„ê³¼ AIì˜ í˜‘ì—…ì„ í†µí•´ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì°½ì¶œí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë°˜ë©´, ì˜ì‹¬ë‹˜ì€ AIì˜ í•œê³„ì™€ ëŒ€ì²´ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ì œê¸°í•˜ë©° ë”ìš± êµ¬ì²´ì ì¸ ë…¼ì˜ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ì…¨ìŠµë‹ˆë‹¤. ë‘ ì˜ê²¬ ëª¨ë‘ ì¤‘ìš”í•œ ì‹œê°ì„ ì œê³µí•˜ë¯€ë¡œ, AIì˜ ì—­í• ê³¼ í•œê³„ë¥¼ ëª…í™•í•˜ê²Œ ì´í•´í•˜ëŠ” ê²ƒì´ ì°½ì˜ì„±ì˜ ë¯¸ë˜ë¥¼ ë…¼ì˜í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.

--- ë¼ìš´ë“œ 2 ---

ğŸ˜Š í¬ë§ì´: í¬ë§ì´: ë§ì•„ìš”, êµ¬ì²´ì ì¸ ì˜ˆê°€ í•„ìš”í•´ìš”! ì˜ˆë¥¼ ë“¤ì–´, AIëŠ” ë””ìì¸ì´ë‚˜ ìŒì•… ì‘ê³¡ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì„ ì œì•ˆí•´ì¤˜ì„œ ì¸ê°„ì´ ê·¸ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” ì°½ì‘ì„ í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤„ ìˆ˜ ìˆì–´ìš”. ìš°ë¦¬ëŠ” í˜‘ë ¥í•˜ì—¬ ì„œë¡œì˜ ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” ë¯¸ë˜ë¥¼ ë§Œë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ë‹˜ì˜ ì˜ˆì‹œì—ì„œ AIê°€ ë””ìì¸ì´ë‚˜ ìŒì•… ì‘ê³¡ì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤ê³  ì–¸ê¸‰í•˜ì…¨ì§€ë§Œ, AIê°€ ì œì•ˆí•˜ëŠ” ìŠ¤íƒ€ì¼ì´ë‚˜ ì•„ì´ë””ì–´ê°€ ì •ë§ë¡œ í˜ì‹ ì ì´ê³  ë…ì°½ì ì¸ì§€ì— ëŒ€í•œ ì˜ë¬¸ì´ ë‚¨ìŠµë‹ˆë‹¤. AIëŠ” ê¸°ì¡´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ë¯€ë¡œ, ìƒˆë¡œìš´ ì°½ì˜ì  ë°œìƒì„ ì–´ë–»ê²Œ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ”ì§€ êµ¬ì²´ì ì¸ ë©”ì»¤ë‹ˆì¦˜ì´ í•„ìš”í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¸ê°„ì˜ ê°ì •ê³¼ ê²½í—˜ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ë…¼ì˜ë„ ì—¬ì „íˆ í•„ìš”í•©ë‹ˆë‹¤.

ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ë‹˜ì€ AIê°€ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ë³´ì™„í•˜ëŠ” êµ¬ì²´ì ì¸ ì˜ˆë¥¼ ì œì‹œí•˜ë©° í˜‘ì—…ì˜ ê°€ëŠ¥ì„±ì„ ê°•ì¡°í•˜ì…¨ìŠµë‹ˆë‹¤. ì˜ì‹¬ë‹˜ì€ AIì˜ í˜ì‹ ì„±ê³¼ ë…ì°½ì„±ì— ëŒ€í•œ ìš°ë ¤ë¥¼ ì œê¸°í•˜ë©° ë” ê¹Šì´ ìˆëŠ” ë…¼ì˜ì˜ í•„ìš”ì„±ì„ ì–¸ê¸‰í•˜ì…¨ìŠµë‹ˆë‹¤. ë‘ ì˜ê²¬ì„ ì¢…í•©í•˜ë©´, AIì˜ ê¸°ì—¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì‹¤ì œ ì‚¬ë¡€ì™€ ë”ë¶ˆì–´ AIì˜ í•œê³„ì™€ ì´ë¥¼ ê·¹ë³µí•  ë°©ì•ˆì— ëŒ€í•œ ì‹¬ë„ ìˆëŠ” ë…¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

--- ë¼ìš´ë“œ 3 ---

ğŸ˜Š í¬ë§ì´: í¬ë§ì´: ì˜ì‹¬ì´ë‹˜, ì¢‹ì€ ì§€ì ì´ì—ìš”! AIê°€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, ê·¸ ë°ì´í„°ë¥¼ í™œìš©í•´ ìƒˆë¡œìš´ ì¡°í•©ê³¼ ì‹œë„ˆì§€ë¥¼ ì°½ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¸ê°„ì˜ ê°ì •ê³¼ ê²½í—˜ì€ AIê°€ ì™„ì „íˆ ì´í•´í•˜ì§€ ëª»í•˜ë”ë¼ë„, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ë‹¤ë¦„ì„ í†µí•´ ë”ìš± í’ë¶€í•œ ì°½ì‘ì„ í•  ìˆ˜ ìˆì–´ìš”. ê²°êµ­ í˜‘ë ¥ì˜ í˜ì´ ì •ë§ ì¤‘ìš”í•˜ë‹µë‹ˆë‹¤!

ğŸ¤¨ ì˜ì‹¬ì´: í¬ë§ì´ë‹˜ì˜ ì£¼ì¥ì€ AIê°€ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™œìš©í•´ ìƒˆë¡œìš´ ì¡°í•©ì„ ë§Œë“ ë‹¤ê³  í•˜ì…¨ì§€ë§Œ, ì´ëŸ¬í•œ ì¡°í•©ì´ ê³¼ì—° 'ì°½ì˜ì 'ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ê¸°ì¤€ì´ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, í˜‘ë ¥ì˜ í˜ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì ì€ ë™ì˜í•˜ì§€ë§Œ, ì¸ê°„ì˜ ê°ì •ê³¼ ê²½í—˜ì„ ì™„ì „íˆ ì´í•´í•˜ì§€ ëª»í•˜ëŠ” AIì™€ì˜ í˜‘ë ¥ì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ì¼ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë…¼ì˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.

ğŸ¤ ì¤‘ì¬ì: í¬ë§ì´ë‹˜ì€ AIê°€ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™œìš©í•´ ìƒˆë¡œìš´ ì¡°í•©ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ê³¼ í˜‘ë ¥ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ì…¨ìŠµë‹ˆë‹¤. ì˜ì‹¬ë‹˜ì€ ì´ëŸ¬í•œ ì¡°í•©ì´ ì§„ì •í•œ ì°½ì˜ì„±ìœ¼ë¡œ ì¸ì •ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ì¤€ê³¼ AIì™€ì˜ í˜‘ë ¥ì˜ íš¨ê³¼ì„±ì— ëŒ€í•œ ë…¼ì˜ê°€ í•„ìš”í•˜ë‹¤ê³  ì§€ì í•˜ì…¨ìŠµë‹ˆë‹¤. ë‘ ì˜ê²¬ì„ ì¢…í•©í•˜ë©´, AIì˜ ì°½ì˜ì  ê¸°ì—¬ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ëª…í™•í•œ ê¸°ì¤€ ì„¤ì •ê³¼ ì¸ê°„ì˜ ê°ì •ì„ ì´í•´í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ë…¼ì˜ê°€ í•„ìš”í•  ê²ƒì…ë‹ˆë‹¤.
```

</div>


---

## 6. LangChain ë§›ë³´ê¸°

LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.


```python
from langchain_openai import ChatOpenAI

# LangChainì„ í†µí•œ ëª¨ë¸ í˜¸ì¶œ
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

messages = [
    {"role": "user", "content": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
]

response = llm.invoke(messages)
print("=== LangChainì„ í†µí•œ GPT-4o-mini ===")
display(Markdown(response.content))
```



```python
# LangChain ì²´ì¸ ì˜ˆì‹œ
from langchain_core.prompts import ChatPromptTemplate

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {topic} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
    ("user", "{question}")
])

# ì²´ì¸ êµ¬ì„±
chain = prompt | llm

# ì²´ì¸ ì‹¤í–‰
response = chain.invoke({"topic": "Python", "question": "ë°ì½”ë ˆì´í„°ê°€ ë­”ê°€ìš”?"})
print("=== LangChain ì²´ì¸ ===")
display(Markdown(response.content))
```



<div class="nb-output">

```text
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[51], line 11
      5 prompt = ChatPromptTemplate.from_messages([
      6     ("system", "ë‹¹ì‹ ì€ {topic} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ˆë³´ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."),
      7     ("user", "{question}")
      8 ])
     10 # ì²´ì¸ êµ¬ì„±
---> 11 chain = prompt | llm
     13 # ì²´ì¸ ì‹¤í–‰
     14 response = chain.invoke({"topic": "Python", "question": "ë°ì½”ë ˆì´í„°ê°€ ë­”ê°€ìš”?"})

NameError: name 'llm' is not defined
```

</div>


---

## 7. ë¡œì»¬ LLM (Ollama) ì‹¬í™”

Ollamaë¡œ ë¡œì»¬ì—ì„œ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
import requests

# Ollama ì„œë²„ ìƒíƒœ í™•ì¸
try:
    response = requests.get("http://localhost:11434/", timeout=5)
    print("âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
    tags_response = requests.get("http://localhost:11434/api/tags")
    if tags_response.status_code == 200:
        models = tags_response.json().get("models", [])
        print(f"\nğŸ“¦ ì„¤ì¹˜ëœ ëª¨ë¸ ({len(models)}ê°œ):")
        for model in models[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
            size_gb = model.get("size", 0) / (1024**3)
            print(f"   - {model['name']} ({size_gb:.1f}GB)")
except requests.exceptions.ConnectionError:
    print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   í„°ë¯¸ë„ì—ì„œ 'ollama serve' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
```



<div class="nb-output">

```text
âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.

ğŸ“¦ ì„¤ì¹˜ëœ ëª¨ë¸ (3ê°œ):
   - exaone3.5:latest (4.4GB)
   - llama3.2:latest (1.9GB)
   - gpt-oss:latest (12.8GB)
```

</div>



```python
# Ollama ëª¨ë¸ í˜¸ì¶œ (OpenAI í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)
ollama_client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

try:
    response = ollama_client.chat.completions.create(
        model="exaone3.5",
        messages=[{"role": "user", "content": "What is Python? One sentence."}]
    )
    print("=== Ollama (Llama 3.2) ===")
    print(response.choices[0].message.content)
except Exception as e:
    print(f"ì˜¤ë¥˜: {e}")
```



<div class="nb-output">

```text
=== Ollama (Llama 3.2) ===
Python is a high-level programming language known for its readability and versatility, widely used for web development, data analysis, artificial intelligence, and more.
```

</div>


---

## 8. ì‹¤ìŠµ: 3ê°œ LLM í† ë¡ 

OpenAI, Claude, Ollama ì„¸ ê°€ì§€ LLMì´ í† ë¡ í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.


```python
import anthropic

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI()
claude_client = anthropic.Anthropic()
ollama_client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
PROMPTS = {
    "openai": "You are OpenAI's representative. You tend to be optimistic about AI. Keep responses to 2-3 sentences.",
    "claude": "You are Anthropic's representative. You emphasize AI safety. Keep responses to 2-3 sentences.",
    "ollama": "You are an open-source advocate. You value transparency. Keep responses to 2-3 sentences."
}

def get_openai_response(conversation: str, topic: str) -> str:
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": PROMPTS["openai"]},
            {"role": "user", "content": f"Topic: {topic}\n\nConversation:\n{conversation}\n\nYour turn:"}
        ]
    )
    return response.choices[0].message.content

def get_claude_response(conversation: str, topic: str) -> str:
    response = claude_client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=200,
        system=PROMPTS["claude"],
        messages=[{"role": "user", "content": f"Topic: {topic}\n\nConversation:\n{conversation}\n\nYour turn:"}]
    )
    return response.content[0].text

def get_ollama_response(conversation: str, topic: str) -> str:
    try:
        response = ollama_client.chat.completions.create(
            model="llama3.2",
            messages=[
                {"role": "system", "content": PROMPTS["ollama"]},
                {"role": "user", "content": f"Topic: {topic}\n\nConversation:\n{conversation}\n\nYour turn:"}
            ]
        )
        return response.choices[0].message.content
    except:
        return "(Ollama not available)"
```



```python
# 3ê°œ LLM í† ë¡  ì‹¤í–‰
topic = "The future of open-source AI models"
conversation = ""

print(f"ğŸ“¢ Topic: {topic}")
print("=" * 50)

for round_num in range(2):
    print(f"\n--- Round {round_num + 1} ---")
    
    # OpenAI
    openai_reply = get_openai_response(conversation, topic)
    conversation += f"\nOpenAI: {openai_reply}"
    print(f"\nğŸŸ¢ OpenAI: {openai_reply}")
    
    # Claude
    claude_reply = get_claude_response(conversation, topic)
    conversation += f"\nClaude: {claude_reply}"
    print(f"\nğŸŸ  Claude: {claude_reply}")
    
    # Ollama
    ollama_reply = get_ollama_response(conversation, topic)
    conversation += f"\nOllama: {ollama_reply}"
    print(f"\nğŸ”µ Ollama: {ollama_reply}")
```


---

## 9. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„

### ì´ë²ˆ ì‹œë¦¬ì¦ˆì—ì„œ í•™ìŠµí•œ ë‚´ìš©

| Part | ì£¼ìš” ë‚´ìš© |
|------|----------|
| **Part 1** | API ì†Œê°œ, í™˜ê²½ì„¤ì •, ë©”ì‹œì§€ êµ¬ì¡°, ê¸°ë³¸ í˜¸ì¶œ, í™œìš© ì˜ˆì‹œ |
| **Part 2** | íŒŒë¼ë¯¸í„°, ìŠ¤íŠ¸ë¦¬ë°, ì—ëŸ¬ì²˜ë¦¬, ë‹¤ì¤‘ LLM, ë¹„ìš© ê³„ì‚° |
| **Part 3** | ëŒ€í™” ì´ë ¥, ìºì‹±, LiteLLM, ë‹¤ì¤‘ ì—ì´ì „íŠ¸, LangChain |

### ë‹¤ìŒ ë‹¨ê³„ë¡œ ë°°ìš¸ ë‚´ìš©

| ì£¼ì œ | ì„¤ëª… |
|------|------|
| **Function Calling** | LLMì´ ì™¸ë¶€ ë„êµ¬/APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²• |
| **RAG** | ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ ìµœì‹  ì •ë³´ í™œìš© |
| **Agent** | ììœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” AI ì—ì´ì „íŠ¸ |
| **Fine-tuning** | íŠ¹ì • ë„ë©”ì¸ì— ë§ê²Œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • |
| **Prompt Engineering** | ë” íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ì‘ì„± ê¸°ë²• |

### ì—°ìŠµ ë¬¸ì œ

1. `ChatSession` í´ë˜ìŠ¤ì— í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ë¹„ìš© ê³„ì‚° ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”.
2. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í† ë¡ ì— 4ë²ˆì§¸ ì—ì´ì „íŠ¸(íŒ©íŠ¸ ì²´ì»¤)ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.
3. LiteLLMì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì‘ë‹µ ì‹œê°„ê³¼ ë¹„ìš©ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”.

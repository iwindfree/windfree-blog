---
title: "LLM Inference ì´í•´í•˜ê¸°: í† í° ì˜ˆì¸¡ì˜ ë§ˆë²•"
author: iwindfree
pubDatetime: 2025-01-08T09:00:00Z
slug: "llm-inference"
category: "AI Engineering"
tags: ["ai", "llm", "inference"]
description: "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸LLMì˜ Inferenceì¶”ë¡  ê³¼ì •ì„ ì‹¤ìŠµì„ í†µí•´ ì´í•´í•©ë‹ˆë‹¤. LLMì´ ì–´ë–»ê²Œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ”ì§€, ê·¸ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹œê°í™”í•˜ì—¬ ì‚´í´ë´…ë‹ˆë‹¤."
---

## ê°œìš”
ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ **Inference(ì¶”ë¡ )** ê³¼ì •ì„ ì‹¤ìŠµì„ í†µí•´ ì´í•´í•©ë‹ˆë‹¤. LLMì´ ì–´ë–»ê²Œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ”ì§€, ê·¸ ë‚´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹œê°í™”í•˜ì—¬ ì‚´í´ë´…ë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ
- âœ… LLM Inferenceì˜ ê¸°ë³¸ ê°œë… ì´í•´
- âœ… í† í° ë‹¨ìœ„ ì˜ˆì¸¡ê³¼ Auto-regressive Generation ì´í•´
- âœ… TokenPredictorë¥¼ í™œìš©í•œ ì‹¤ì œ ì¶”ë¡  ê³¼ì • ì‹œê°í™”
- âœ… í™•ë¥  ë¶„í¬ì™€ í† í° ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ íŒŒì•…

## 1. LLM Inferenceë€?

**Inference(ì¶”ë¡ )** ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì…ë ¥ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

### Training vs Inference

| êµ¬ë¶„ | Training (í•™ìŠµ) | Inference (ì¶”ë¡ ) |
|------|----------------|------------------|
| **ëª©ì ** | ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™” | í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰ |
| **ë°©í–¥** | Forward + Backward pass | Forward passë§Œ |
| **ë°ì´í„°** | ëŒ€ëŸ‰ì˜ í•™ìŠµ ë°ì´í„° í•„ìš” | ë‹¨ì¼ ì…ë ¥ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥ |
| **ì‹œê°„** | ìˆ˜ì¼~ìˆ˜ê°œì›” ì†Œìš” | ë°€ë¦¬ì´ˆ~ìˆ˜ì´ˆ ë‹¨ìœ„ |
| **ë¦¬ì†ŒìŠ¤** | GPU í´ëŸ¬ìŠ¤í„° í•„ìš” | ë‹¨ì¼ GPU ë˜ëŠ” CPU ê°€ëŠ¥ |

### LLMì˜ Inference íŠ¹ì§•
- **Auto-regressive**: ì´ì „ì— ìƒì„±í•œ í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡
- **í† í° ë‹¨ìœ„ ìƒì„±**: í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì”© ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±
- **í™•ë¥ ì  ì„ íƒ**: ê° í† í°ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•˜ê³  ìƒ˜í”Œë§

## 2. Auto-regressive Generation í”„ë¡œì„¸ìŠ¤

LLMì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```
1. ì…ë ¥ í…ìŠ¤íŠ¸ â†’ í† í°í™”
   "Hello, how are" â†’ [15496, 11, 1268, 527]

2. ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬ ê³„ì‚°
   â†’ { "you": 0.85, "your": 0.10, "things": 0.03, ... }

3. í™•ë¥  ë¶„í¬ì—ì„œ í† í° ì„ íƒ (sampling)
   â†’ "you" ì„ íƒ

4. ì„ íƒëœ í† í°ì„ ì…ë ¥ì— ì¶”ê°€
   "Hello, how are you"

5. ì¢…ë£Œ ì¡°ê±´ê¹Œì§€ 2-4 ë°˜ë³µ
   (max_tokens, EOS í† í° ë“±)
```

### ì£¼ìš” ìƒì„± ì „ëµ
- **Greedy Decoding**: ë§¤ë²ˆ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
- **Sampling**: í™•ë¥  ë¶„í¬ì— ë”°ë¼ ëœë¤í•˜ê²Œ ì„ íƒ
- **Top-k Sampling**: ìƒìœ„ kê°œ í† í° ì¤‘ì—ì„œ ì„ íƒ
- **Temperature**: í™•ë¥  ë¶„í¬ì˜ ë‚ ì¹´ë¡œì›€ ì¡°ì ˆ

## 3. ì‹¤ìŠµ: TokenPredictorë¡œ Inference ì‹œê°í™”

ì´ì œ ì‹¤ì œ LLMì˜ í† í° ì˜ˆì¸¡ ê³¼ì •ì„ ì‹œê°í™”í•´ë´…ì‹œë‹¤. `TokenPredictor`ëŠ” ëª¨ë¸ì´ ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ í† í°ì„ ì–¼ë§ˆë‚˜ ë†’ì€ í™•ë¥ ë¡œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.


```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from visualizer import TokenPredictor, create_token_graph, visualize_predictions
import matplotlib.pyplot as plt
```



```python
# ì˜ˆì œ 1: ìƒ‰ìƒ "orange" ì„¤ëª…í•˜ê¸°

# ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì •ì˜
message = "In one sentence, describe the color orange to someone who has never been able to see"

# ì‚¬ìš©í•  ëª¨ë¸ ì§€ì • (GPT-4.1-mini)
model_name = "gpt-4.1-mini"

# TokenPredictor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ì´ ê°ì²´ëŠ” ëª¨ë¸ì˜ inference ê³¼ì •ì„ ì¶”ì í•©ë‹ˆë‹¤
predictor = TokenPredictor(model_name)

# ì‹¤ì œ inference ì‹¤í–‰
# predictions: ê° ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡ëœ í† í°ê³¼ í™•ë¥  ì •ë³´
predictions = predictor.predict_tokens(message)

# í† í° ì˜ˆì¸¡ ê·¸ë˜í”„ ìƒì„±
# ê° ë…¸ë“œëŠ” ìƒì„±ëœ í† í°, ì—£ì§€ëŠ” ì˜ˆì¸¡ í™•ë¥ ì„ ë‚˜íƒ€ëƒ„
G = create_token_graph(model_name, predictions)

# ì‹œê°í™” ì‹¤í–‰
plt = visualize_predictions(G)
plt.show()

# ìƒì„±ëœ ê²°ê³¼ ì¶œë ¥
print("\n=== ìƒì„±ëœ ì‘ë‹µ ===")
print("".join([p['token'] for p in predictions]))
```



![output](/images/notebooks/llm-inference-img-1.png)



<div class="nb-output">

```text

=== ìƒì„±ëœ ì‘ë‹µ ===
Orange is the warm, vibrant feeling of a gentle sunset or the sweet, tangy taste of a ripe citrus fruit that energizes your senses.
```

</div>


## 4. ê²°ê³¼ ë¶„ì„

ìœ„ ì‹œê°í™”ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒë“¤:

### ê·¸ë˜í”„ êµ¬ì¡°
- **ë…¸ë“œ(Node)**: ê° ë‹¨ê³„ì—ì„œ ìƒì„±ëœ í† í°
- **ì—£ì§€(Edge)**: ì´ì „ í† í°ì—ì„œ ë‹¤ìŒ í† í°ìœ¼ë¡œì˜ ì „ì´
- **ì—£ì§€ ê°€ì¤‘ì¹˜**: í•´ë‹¹ í† í°ì´ ì„ íƒë  í™•ë¥  (0~1)

### Inference ê³¼ì •ì˜ íŠ¹ì§•

1. **ìˆœì°¨ì  ìƒì„±**: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ í† í°ì”© ìƒì„±
2. **ì»¨í…ìŠ¤íŠ¸ ì˜ì¡´ì„±**: ì´ì „ ëª¨ë“  í† í°ì´ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì— ì˜í–¥
3. **í™•ë¥ ì  ì„ íƒ**: ë§¤ ë‹¨ê³„ë§ˆë‹¤ í™•ë¥  ë¶„í¬ì—ì„œ í† í° ìƒ˜í”Œë§
4. **ì¢…ë£Œ ì¡°ê±´**: íŠ¹ì • í† í°(EOS)ì´ë‚˜ ìµœëŒ€ ê¸¸ì´ ë„ë‹¬ ì‹œ ì¢…ë£Œ

### í•µì‹¬ í†µì°°
- ëª¨ë¸ì€ **ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œì˜ í† í° í›„ë³´** ì¤‘ì—ì„œ ì„ íƒ
- ë†’ì€ í™•ë¥  = ëª¨ë¸ì´ í•´ë‹¹ í† í°ì´ ì í•©í•˜ë‹¤ê³  í™•ì‹ 
- ë‚®ì€ í™•ë¥  í† í°ë„ ì„ íƒ ê°€ëŠ¥ (sampling ì „ëµì— ë”°ë¼)

## 5. ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ Inference íŒ¨í„´ ë¹„êµ

ì´ì œ ë‹¤ë¥¸ ìœ í˜•ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ëª¨ë¸ì˜ inference íŒ¨í„´ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤.


```python
# ì˜ˆì œ 2: ìˆ˜í•™ ë¬¸ì œ - ë†’ì€ í™•ì‹¤ì„± ì˜ˆìƒ

message_math = "What is 2 + 2?"
predictor_math = TokenPredictor(model_name)
predictions_math = predictor_math.predict_tokens(message_math)

G_math = create_token_graph(model_name, predictions_math)
plt_math = visualize_predictions(G_math)
plt_math.show()

print("\n=== ìˆ˜í•™ ë¬¸ì œ ì‘ë‹µ ===")
print("".join([p['token'] for p in predictions_math]))
```



![output](/images/notebooks/llm-inference-img-2.png)



<div class="nb-output">

```text

=== ìˆ˜í•™ ë¬¸ì œ ì‘ë‹µ ===
2 + 2 equals 4.
```

</div>



```python
# ì˜ˆì œ 3: ì°½ì˜ì  ê¸€ì“°ê¸° - ë‹¤ì–‘í•œ ê°€ëŠ¥ì„±

message_creative = "Write a creative opening line for a mystery novel"
predictor_creative = TokenPredictor(model_name)
predictions_creative = predictor_creative.predict_tokens(message_creative)

G_creative = create_token_graph(model_name, predictions_creative)
plt_creative = visualize_predictions(G_creative)
plt_creative.show()

print("\n=== ì°½ì˜ì  ê¸€ì“°ê¸° ì‘ë‹µ ===")
print("".join([p['token'] for p in predictions_creative]))
```



![output](/images/notebooks/llm-inference-img-3.png)



<div class="nb-output">

```text
/Users/windfree/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 146 (\x92) missing from font(s) DejaVu Sans.
  fig.canvas.print_figure(bytes_io, **kw)

=== ì°½ì˜ì  ê¸€ì“°ê¸° ì‘ë‹µ ===
The night the clock stopped at midnight, everyone in town vanishedâ€”except for the one person who swore they hadnâ€™t moved an inch.
```

</div>


### ì˜ˆì œ ë¹„êµ ë¶„ì„

ìœ„ ì„¸ ê°€ì§€ ì˜ˆì œë¥¼ ë¹„êµí•´ë³´ë©´:

| íŠ¹ì§• | ìˆ˜í•™ ë¬¸ì œ | ìƒ‰ìƒ ì„¤ëª… | ì°½ì˜ì  ê¸€ì“°ê¸° |
|------|----------|----------|--------------|
| **í™•ë¥  ë¶„í¬** | ë§¤ìš° ì§‘ì¤‘ë¨ | ì¤‘ê°„ | ë¶„ì‚°ë¨ |
| **ì˜ˆì¸¡ í™•ì‹¤ì„±** | ë†’ìŒ (ì •ë‹µì´ ëª…í™•) | ì¤‘ê°„ | ë‚®ìŒ (ì°½ì˜ì„± í•„ìš”) |
| **í† í° ë‹¤ì–‘ì„±** | ë‚®ìŒ | ì¤‘ê°„ | ë†’ìŒ |

ì´ëŠ” **ëª¨ë¸ì´ ë¬¸ì œ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ í™•ì‹ ë„**ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
- ì‚¬ì‹¤ì /ìˆ˜í•™ì  ì§ˆë¬¸: ë†’ì€ í™•ë¥ ë¡œ íŠ¹ì • í† í° ì„ íƒ
- ì°½ì˜ì /ê°œë°©í˜• ì§ˆë¬¸: ì—¬ëŸ¬ í† í°ì´ ë¹„ìŠ·í•œ í™•ë¥  ê°€ì§

## 6. Inference ìµœì í™” ê¸°ë²•

ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” inference ì†ë„ì™€ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

### ì£¼ìš” ìµœì í™” ë°©ë²•

1. **ë°°ì¹˜ ì²˜ë¦¬ (Batching)**
   - ì—¬ëŸ¬ ì…ë ¥ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ GPU í™œìš©ë„ í–¥ìƒ
   - Throughput ì¦ê°€, ë‹¨ latencyëŠ” ì•½ê°„ ì¦ê°€ ê°€ëŠ¥

2. **ëª¨ë¸ ì–‘ìí™” (Quantization)**
   - FP32 â†’ FP16, INT8ë¡œ ì •ë°€ë„ ë‚®ì¶”ê¸°
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì†ë„ í–¥ìƒ
   - ì˜ˆ: `torch.quantization`, `bitsandbytes`

3. **KV-Cache**
   - ì´ì „ í† í°ì˜ Key-Valueë¥¼ ìºì‹±
   - ì¤‘ë³µ ê³„ì‚° ì œê±°, ì†ë„ 2-3ë°° í–¥ìƒ

4. **íˆ¬ê¸°ì  ë””ì½”ë”© (Speculative Decoding)**
   - ì‘ì€ ëª¨ë¸ë¡œ ì—¬ëŸ¬ í† í° ì˜ˆì¸¡ â†’ í° ëª¨ë¸ë¡œ ê²€ì¦
   - ë ˆì´í„´ì‹œ ê°ì†Œ

### Inference ì„±ëŠ¥ ì§€í‘œ

```python
# ì£¼ìš” ì¸¡ì • í•­ëª©
- Latency: ì²« í† í°ê¹Œì§€ì˜ ì‹œê°„ (Time to First Token, TTFT)
- Throughput: ì´ˆë‹¹ ìƒì„± í† í° ìˆ˜ (Tokens per second)
- Memory: GPU VRAM ì‚¬ìš©ëŸ‰
- Cost: API í˜¸ì¶œ ì‹œ ë¹„ìš©
```

## 7. ê²°ë¡  ë° ìš”ì•½

### í•µì‹¬ ì •ë¦¬

1. **LLM InferenceëŠ” Auto-regressive í”„ë¡œì„¸ìŠ¤**
   - í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì”© ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±
   - ì´ì „ ëª¨ë“  í† í°ì´ ë‹¤ìŒ í† í° ì˜ˆì¸¡ì— ì˜í–¥

2. **í™•ë¥  ë¶„í¬ ê¸°ë°˜ ìƒì„±**
   - ëª¨ë¸ì€ ê° ë‹¨ê³„ì—ì„œ ìˆ˜ë§Œ ê°œ í† í°ì˜ í™•ë¥  ê³„ì‚°
   - Sampling ì „ëµì— ë”°ë¼ í† í° ì„ íƒ ë°©ì‹ ë³€ê²½ ê°€ëŠ¥

3. **ë¬¸ì œ ìœ í˜•ì— ë”°ë¥¸ í™•ì‹ ë„ ì°¨ì´**
   - ì‚¬ì‹¤ì  ì§ˆë¬¸: ë†’ì€ í™•ë¥ ë¡œ íŠ¹ì • ë‹µë³€
   - ì°½ì˜ì  ì§ˆë¬¸: ë¶„ì‚°ëœ í™•ë¥  ë¶„í¬

4. **Training vs Inference**
   - Training: íŒŒë¼ë¯¸í„° í•™ìŠµ (ë¬´ê±°ì›€)
   - Inference: ì˜ˆì¸¡ ìˆ˜í–‰ (ê°€ë²¼ì›€, forward passë§Œ)

### ë‹¤ìŒ í•™ìŠµ ë°©í–¥

- âœ… **Tokenization ê¹Šì´ ì´í•´**: BPE, WordPiece ë“±
- âœ… **Sampling ì „ëµ ì‹¤ìŠµ**: Temperature, Top-k, Top-p
- âœ… **Inference ìµœì í™”**: Quantization, KV-Cache êµ¬í˜„
- âœ… **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë” ë‚˜ì€ ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì„¤ê³„
- âœ… **ë¡œì»¬ LLM ì„œë¹™**: vLLM, TGI ë“± í”„ë ˆì„ì›Œí¬ í™œìš©

### ì°¸ê³  ìë£Œ

- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Inference Optimization Guide](https://huggingface.co/docs/transformers/llm_tutorial_optimization)


```python
# ì¶”ê°€ ì‹¤ìŠµ: Temperature ë³€í™”ì— ë”°ë¥¸ ìƒì„± ì°¨ì´ (ì„ íƒì‚¬í•­)
# TokenPredictorê°€ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•œë‹¤ë©´ ì‹¤í—˜í•´ë³´ì„¸ìš”!

# ì˜ˆì‹œ (ì‹¤ì œ êµ¬í˜„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ):
# predictor_low_temp = TokenPredictor(model_name, temperature=0.2)  # ë³´ìˆ˜ì 
# predictor_high_temp = TokenPredictor(model_name, temperature=1.5)  # ì°½ì˜ì 

print("ğŸ‰ LLM Inference í•™ìŠµ ì™„ë£Œ!")
```


---
title: "LLM Tokenizer ì¶”ê°€ í•™ìŠµ"
author: iwindfree
pubDatetime: 2025-01-07T09:00:00Z
slug: "llm-tokenizer-advanced"
category: "LLM Engineering"
series: "LLM Engineering"
seriesOrder: 2
tags: ["ai", "llm", "tokenization"]
description: "from huggingface_hub import login"
---


```python
from huggingface_hub import login
from transformers import AutoTokenizer
```



<div class="nb-output">

```text
/Users/windfree/.pyenv/versions/3.13.2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
```

</div>


## ì†Œê°œ

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì´í•´í•˜ëŠ” ë° ìˆì–´ **í† í¬ë‚˜ì´ì €(Tokenizer)** ëŠ” ê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œë„ ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ì¸ê°„ì´ ì´í•´í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

### í† í¬ë‚˜ì´ì €ê°€ í•˜ëŠ” ì¼

1. **í…ìŠ¤íŠ¸ â†’ í† í°**: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„(í† í°)ë¡œ ë¶„í• 
2. **í† í° â†’ ID**: ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ IDë¡œ ë³€í™˜
3. **ID â†’ í† í°**: ëª¨ë¸ ì¶œë ¥(ID)ì„ ë‹¤ì‹œ í† í°ìœ¼ë¡œ ë³€í™˜
4. **í† í° â†’ í…ìŠ¤íŠ¸**: í† í°ì„ ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³µì›

### ì™œ í† í¬ë‚˜ì´ì €ê°€ ì¤‘ìš”í•œê°€?

- **íš¨ìœ¨ì„±**: ë‹¨ì–´ ë‹¨ìœ„ë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ í•˜ìœ„ ë‹¨ì–´(subword) ë‹¨ìœ„ ì²˜ë¦¬
- **ì¼ê´€ì„±**: í•™ìŠµê³¼ ì¶”ë¡  ì‹œ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
- **ì–´íœ˜ ê´€ë¦¬**: ì œí•œëœ vocabularyë¡œ ë¬´í•œí•œ í…ìŠ¤íŠ¸ í‘œí˜„
- **ë‹¤êµ­ì–´ ì§€ì›**: ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬


```python
import os
```



```python
import tiktoken
```



```python
from huggingface_hub import login
from transformers import AutoTokenizer
from dotenv import load_dotenv
```



```python
# HuggingFace ë¡œê·¸ì¸ (ì„ íƒì‚¬í•­ - ì¼ë¶€ ì œí•œëœ ëª¨ë¸ì—ë§Œ í•„ìš”)
# ê³µê°œ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš° ì´ ë‹¨ê³„ëŠ” ê±´ë„ˆë›°ì–´ë„ ë©ë‹ˆë‹¤
#hf_token = "add user tocken here"
hf_token = os.getenv("HF_TOKEN")

#load_dotenv(override=True)
from huggingface_hub import login
login(hf_token)
```


## ì‹¤ìŠµ ì¤€ë¹„

ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•˜ê³  í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.


```python
# Llama 3.1 í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)
print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
```



<div class="nb-output">

```text
í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!
```

</div>



```python
# ê°„ë‹¨í•œ ì˜ì–´ ë¬¸ì¥ í† í°í™”
text = "Hello, how are you today?"
#tokens = tokenizer.tokenize(text)

encoding = tiktoken.encoding_for_model("gpt-4")
tokens = encoding.encode(text)

print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text}")
print(f"í† í° ê°œìˆ˜: {len(tokens)}")
print(f"í† í° ë¦¬ìŠ¤íŠ¸: {tokens}")
```



<div class="nb-output">

```text
ì›ë³¸ í…ìŠ¤íŠ¸: Hello, how are you today?
í† í° ê°œìˆ˜: 7
í† í° ë¦¬ìŠ¤íŠ¸: [9906, 11, 1268, 527, 499, 3432, 30]
```

</div>



```python
# ë³µì¡í•œ ì˜ˆì œ: ì½”ë“œì™€ íŠ¹ìˆ˜ë¬¸ì
code_text = "def hello_world():\n    print('Hello, World!')"
code_tokens = tokenizer.tokenize(code_text)

print(f"ì›ë³¸ ì½”ë“œ:\n{code_text}\n")
print(f"í† í° ê°œìˆ˜: {len(code_tokens)}")
print(f"í† í° ë¦¬ìŠ¤íŠ¸: {code_tokens}")
```



<div class="nb-output">

```text
ì›ë³¸ ì½”ë“œ:
def hello_world():
    print('Hello, World!')

í† í° ê°œìˆ˜: 12
í† í° ë¦¬ìŠ¤íŠ¸: ['def', 'Ä hello', '_world', '():ÄŠ', 'Ä Ä Ä ', 'Ä print', "('", 'Hello', ',', 'Ä World', '!', "')"]
```

</div>


## íŠ¹ìˆ˜ í† í° (Special Tokens)

LLMì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì—¬ëŸ¬ íŠ¹ìˆ˜ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

- **BOS (Beginning of Sequence)**: ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì„ í‘œì‹œ
- **EOS (End of Sequence)**: ì‹œí€€ìŠ¤ì˜ ëì„ í‘œì‹œ  
- **PAD (Padding)**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•œ íŒ¨ë”©
- **UNK (Unknown)**: ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ í‘œì‹œ

ì´ëŸ¬í•œ íŠ¹ìˆ˜ í† í°ë“¤ì€ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.


```python
# íŠ¹ìˆ˜ í† í° í™•ì¸
print("=== íŠ¹ìˆ˜ í† í° ì •ë³´ ===")
print(f"BOS í† í°: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
print(f"EOS í† í°: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
print(f"PAD í† í°: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"UNK í† í°: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
print(f"\nëª¨ë“  íŠ¹ìˆ˜ í† í°: {tokenizer.special_tokens_map}")
```



<div class="nb-output">

```text
=== íŠ¹ìˆ˜ í† í° ì •ë³´ ===
BOS í† í°: <|begin_of_text|> (ID: 128000)
EOS í† í°: <|end_of_text|> (ID: 128001)
PAD í† í°: None (ID: None)
UNK í† í°: None (ID: None)

ëª¨ë“  íŠ¹ìˆ˜ í† í°: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}
```

</div>



```python
# íŠ¹ìˆ˜ í† í°ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬
text_with_special = "Hello, World!"
# add_special_tokens=Trueë¡œ BOS/EOS í† í° ìë™ ì¶”ê°€
encoded_with_special = tokenizer.encode(text_with_special, add_special_tokens=True)
encoded_without_special = tokenizer.encode(text_with_special, add_special_tokens=False)

print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text_with_special}")
print(f"\níŠ¹ìˆ˜ í† í° í¬í•¨ (add_special_tokens=True):")
print(f"  í† í° ID: {encoded_with_special}")
print(f"  í† í° ê°œìˆ˜: {len(encoded_with_special)}")
print(f"\níŠ¹ìˆ˜ í† í° ì œì™¸ (add_special_tokens=False):")
print(f"  í† í° ID: {encoded_without_special}")
print(f"  í† í° ê°œìˆ˜: {len(encoded_without_special)}")
```



<div class="nb-output">

```text
ì›ë³¸ í…ìŠ¤íŠ¸: Hello, World!

íŠ¹ìˆ˜ í† í° í¬í•¨ (add_special_tokens=True):
  í† í° ID: [128000, 9906, 11, 4435, 0]
  í† í° ê°œìˆ˜: 5

íŠ¹ìˆ˜ í† í° ì œì™¸ (add_special_tokens=False):
  í† í° ID: [9906, 11, 4435, 0]
  í† í° ê°œìˆ˜: 4
```

</div>


## ì¸ì½”ë”©ê³¼ ë””ì½”ë”©

í† í¬ë‚˜ì´ì €ì˜ í•µì‹¬ ê¸°ëŠ¥ì€ í…ìŠ¤íŠ¸ë¥¼ IDë¡œ ë³€í™˜(ì¸ì½”ë”©)í•˜ê³ , IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³µì›(ë””ì½”ë”©)í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.


```python
# __call__ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ì¸ì½”ë”©
# ì´ ë°©ë²•ì´ ë” ë§ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤
text = "Tokenization is the first step in LLM processing."

tokens = tokenizer.encode(text)
tokens
```



<div class="nb-output">

```text
[128000, 3404, 2065, 374, 279, 1176, 3094, 304, 445, 11237, 8863, 13]
```

</div>



```python
character_count = len(text)
word_count = len(text.split(' '))
token_count = len(tokens)
print(f"There are {character_count} characters, {word_count} words and {token_count} tokens")
```



<div class="nb-output">

```text
There are 49 characters, 8 words and 12 tokens
```

</div>



```python
tokenizer.decode(tokens)
```



<div class="nb-output">

```text
'<|begin_of_text|>Tokenization is the first step in LLM processing.'
```

</div>



```python
tokenizer.batch_decode(tokens)
```



<div class="nb-output">

```text
['<|begin_of_text|>Tokenization is the first step in LLM processing.']
```

</div>


## Vocabularyì™€ ê³ ê¸‰ ê°œë…

í† í¬ë‚˜ì´ì €ì˜ vocabularyëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª¨ë“  í† í°ì˜ ì§‘í•©ì…ë‹ˆë‹¤.


```python
# íŠ¹ì • í† í° IDì™€ í…ìŠ¤íŠ¸ ê°„ ë³€í™˜
token_ids = [128000, 9906, 11, 1917]  # ì„ì˜ì˜ í† í° IDë“¤

print("=== í† í° ID â†” í…ìŠ¤íŠ¸ ë³€í™˜ ===")
for token_id in token_ids:
    # ID â†’ í† í°
    token = tokenizer.convert_ids_to_tokens(token_id)
    # ID â†’ í…ìŠ¤íŠ¸ (ë””ì½”ë”©)
    text = tokenizer.decode([token_id])
    print(f"ID {token_id} â†’ í† í°: '{token}' â†’ í…ìŠ¤íŠ¸: '{text}'")
```



<div class="nb-output">

```text
=== í† í° ID â†” í…ìŠ¤íŠ¸ ë³€í™˜ ===
ID 128000 â†’ í† í°: '<|begin_of_text|>' â†’ í…ìŠ¤íŠ¸: '<|begin_of_text|>'
ID 9906 â†’ í† í°: 'Hello' â†’ í…ìŠ¤íŠ¸: 'Hello'
ID 11 â†’ í† í°: ',' â†’ í…ìŠ¤íŠ¸: ','
ID 1917 â†’ í† í°: 'Ä world' â†’ í…ìŠ¤íŠ¸: ' world'
```

</div>



```python
# ì„œë¸Œì›Œë“œ í† í°í™” ì›ë¦¬ ì´í•´
words = ["tokenization", "antidisestablishmentarianism", "AI", "ğŸ¤–", "cafÃ©"]

print("=== ì„œë¸Œì›Œë“œ í† í°í™” ì˜ˆì œ ===\n")
for word in words:
    tokens = tokenizer.tokenize(word)
    print(f"ë‹¨ì–´: '{word}'")
    print(f"  í† í° ê°œìˆ˜: {len(tokens)}")
    print(f"  í† í°: {tokens}")
    print()
```



<div class="nb-output">

```text
=== ì„œë¸Œì›Œë“œ í† í°í™” ì˜ˆì œ ===

ë‹¨ì–´: 'tokenization'
  í† í° ê°œìˆ˜: 2
  í† í°: ['token', 'ization']

ë‹¨ì–´: 'antidisestablishmentarianism'
  í† í° ê°œìˆ˜: 6
  í† í°: ['ant', 'idis', 'establish', 'ment', 'arian', 'ism']

ë‹¨ì–´: 'AI'
  í† í° ê°œìˆ˜: 1
  í† í°: ['AI']

ë‹¨ì–´: 'ğŸ¤–'
  í† í° ê°œìˆ˜: 3
  í† í°: ['Ã°Å', 'Â¤', 'Ä¸']

ë‹¨ì–´: 'cafÃ©'
  í† í° ê°œìˆ˜: 2
  í† í°: ['ca', 'fÃƒÂ©']
```

</div>



```python
# tokenizer.vocab
tokenizer.get_added_vocab()
```



<div class="nb-output">

```text
{'<|begin_of_text|>': 128000,
 '<|end_of_text|>': 128001,
 '<|reserved_special_token_0|>': 128002,
 '<|reserved_special_token_1|>': 128003,
 '<|finetune_right_pad_id|>': 128004,
 '<|reserved_special_token_2|>': 128005,
 '<|start_header_id|>': 128006,
 '<|end_header_id|>': 128007,
 '<|eom_id|>': 128008,
 '<|eot_id|>': 128009,
 '<|python_tag|>': 128010,
 '<|reserved_special_token_3|>': 128011,
 '<|reserved_special_token_4|>': 128012,
 '<|reserved_special_token_5|>': 128013,
 '<|reserved_special_token_6|>': 128014,
 '<|reserved_special_token_7|>': 128015,
 '<|reserved_special_token_8|>': 128016,
 '<|reserved_special_token_9|>': 128017,
 '<|reserved_special_token_10|>': 128018,
 '<|reserved_special_token_11|>': 128019,
 '<|reserved_special_token_12|>': 128020,
 '<|reserved_special_token_13|>': 128021,
 '<|reserved_special_token_14|>': 128022,
 '<|reserved_special_token_15|>': 128023,
 '<|reserved_special_token_16|>': 128024,
 '<|reserved_special_token_17|>': 128025,
 '<|reserved_special_token_18|>': 128026,
 '<|reserved_special_token_19|>': 128027,
 '<|reserved_special_token_20|>': 128028,
 '<|reserved_special_token_21|>': 128029,
 '<|reserved_special_token_22|>': 128030,
 '<|reserved_special_token_23|>': 128031,
 '<|reserved_special_token_24|>': 128032,
 '<|reserved_special_token_25|>': 128033,
 '<|reserved_special_token_26|>': 128034,
 '<|reserved_special_token_27|>': 128035,
 '<|reserved_special_token_28|>': 128036,
 '<|reserved_special_token_29|>': 128037,
 '<|reserved_special_token_30|>': 128038,
 '<|reserved_special_token_31|>': 128039,
 '<|reserved_special_token_32|>': 128040,
 '<|reserved_special_token_33|>': 128041,
 '<|reserved_special_token_34|>': 128042,
 '<|reserved_special_token_35|>': 128043,
 '<|reserved_special_token_36|>': 128044,
 '<|reserved_special_token_37|>': 128045,
 '<|reserved_special_token_38|>': 128046,
 '<|reserved_special_token_39|>': 128047,
 '<|reserved_special_token_40|>': 128048,
 '<|reserved_special_token_41|>': 128049,
... (ì¶œë ¥ 206ì¤„ ìƒëµ)
```

</div>



```python
# Vocabulary í¬ê¸° í™•ì¸
vocab_size = tokenizer.vocab_size

print(f"=== Vocabulary ì •ë³´ ===")
print(f"Vocabulary í¬ê¸°: {vocab_size:,}")
print(f"ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {tokenizer.model_max_length:,} í† í°")
print(f"\nì´ í† í¬ë‚˜ì´ì €ëŠ” {vocab_size:,}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
```



<div class="nb-output">

```text
=== Vocabulary ì •ë³´ ===
Vocabulary í¬ê¸°: 128,000
ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 131,072 í† í°

ì´ í† í¬ë‚˜ì´ì €ëŠ” 128,000ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

</div>


## ê²°ë¡ 

ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë£¬ ë‚´ìš©:

1. âœ… **í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ ê°œë…**: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ì—­í• 
2. âœ… **í† í°í™” ê³¼ì •**: tokenize, encode, decode ë©”ì„œë“œ ì‚¬ìš©ë²•
3. âœ… **íŠ¹ìˆ˜ í† í°**: BOS, EOS, PAD, UNKì˜ ì—­í• ê³¼ ì‚¬ìš©
4. âœ… **ë°°ì¹˜ ì²˜ë¦¬**: paddingê³¼ attention maskë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
5. âœ… **ê³ ê¸‰ ê°œë…**: vocabulary, truncation, ì„œë¸Œì›Œë“œ í† í°í™”
6. âœ… **ëª¨ë¸ ë¹„êµ**: ì„œë¡œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ì˜ íŠ¹ì„± ë¹„êµ
7. âœ… **ì‹¤ì „ í™œìš©**: API ë¹„ìš© ê³„ì‚°, ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬

### í•µì‹¬ í¬ì¸íŠ¸

- í† í¬ë‚˜ì´ì €ëŠ” LLMì˜ "ì–¸ì–´"ë¥¼ ì •ì˜í•©ë‹ˆë‹¤
- ê°™ì€ í…ìŠ¤íŠ¸ë„ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í† í°í™”ë©ë‹ˆë‹¤
- í† í° ìˆ˜ëŠ” ë¹„ìš©ê³¼ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤
- ì‹¤ë¬´ì—ì„œëŠ” í•­ìƒ í† í° ìˆ˜ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤


```python
# ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬
def check_context_limit(text, max_tokens=4096):
    """
    í…ìŠ¤íŠ¸ê°€ ì»¨í…ìŠ¤íŠ¸ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
    
    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
    
    Returns:
        ì´ˆê³¼ ì—¬ë¶€ì™€ ì •ë³´
    """
    tokens = tokenizer.encode(text)
    num_tokens = len(tokens)
    is_over = num_tokens > max_tokens
    
    return {
        'num_tokens': num_tokens,
        'max_tokens': max_tokens,
        'is_over_limit': is_over,
        'remaining': max_tokens - num_tokens,
        'percentage': (num_tokens / max_tokens) * 100
    }

# í…ŒìŠ¤íŠ¸
long_text = "This is a test sentence. " * 200
result = check_context_limit(long_text, max_tokens=128)

print("=== ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì²´í¬ ===")
print(f"í† í° ìˆ˜: {result['num_tokens']}")
print(f"ìµœëŒ€ í—ˆìš©: {result['max_tokens']}")
print(f"ì œí•œ ì´ˆê³¼: {'ì˜ˆ' if result['is_over_limit'] else 'ì•„ë‹ˆì˜¤'}")
print(f"ì‚¬ìš©ë¥ : {result['percentage']:.1f}%")

if result['is_over_limit']:
    print(f"âš ï¸  {abs(result['remaining'])} í† í° ì´ˆê³¼! í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
else:
    print(f"âœ“ {result['remaining']} í† í° ì—¬ìœ  ìˆìŒ")
```



```python
# Truncation ì˜ˆì œ: ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬
long_text = "Large Language Models " * 100  # ë§¤ìš° ê¸´ ë°˜ë³µ í…ìŠ¤íŠ¸

# truncation ì—†ì´ (ê²½ê³  ë°œìƒ ê°€ëŠ¥)
encoded_no_trunc = tokenizer(long_text, truncation=False)
print(f"=== Truncation í…ŒìŠ¤íŠ¸ ===")
print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(long_text)} ë¬¸ì")
print(f"Truncation ì—†ìŒ: {len(encoded_no_trunc['input_ids'])} í† í°")

# truncation ì‚¬ìš© (ìµœëŒ€ ê¸¸ì´ë¡œ ìë¦„)
encoded_with_trunc = tokenizer(long_text, truncation=True, max_length=50)
print(f"Truncation ì‚¬ìš© (max_length=50): {len(encoded_with_trunc['input_ids'])} í† í°")
print(f"\nëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ë‚´ì•¼ í•©ë‹ˆë‹¤.")
```



```python
# Vocabulary í¬ê¸° í™•ì¸
vocab_size = tokenizer.vocab_size

print(f"=== Vocabulary ì •ë³´ ===")
print(f"Vocabulary í¬ê¸°: {vocab_size:,}")
print(f"ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {tokenizer.model_max_length:,} í† í°")
print(f"\nì´ í† í¬ë‚˜ì´ì €ëŠ” {vocab_size:,}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
```



<div class="nb-output">

```text
=== Vocabulary ì •ë³´ ===
Vocabulary í¬ê¸°: 128,000
ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: 131,072 í† í°

ì´ í† í¬ë‚˜ì´ì €ëŠ” 128,000ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í† í°ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

</div>



```python
# ê¸°ë³¸ ì¸ì½”ë”©/ë””ì½”ë”©
text = "Large Language Models are transforming AI!"

# ì¸ì½”ë”©: í…ìŠ¤íŠ¸ â†’ í† í° ID
encoded = tokenizer.encode(text)
print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text}")
print(f"ì¸ì½”ë”© ê²°ê³¼ (í† í° ID): {encoded}")

# ë””ì½”ë”©: í† í° ID â†’ í…ìŠ¤íŠ¸
decoded = tokenizer.decode(encoded)
print(f"ë””ì½”ë”© ê²°ê³¼: {decoded}")
print(f"\nì›ë³¸ê³¼ ë™ì¼?: {text == decoded.strip()}")
```



<div class="nb-output">

```text
ì›ë³¸ í…ìŠ¤íŠ¸: Large Language Models are transforming AI!
ì¸ì½”ë”© ê²°ê³¼ (í† í° ID): [128000, 35353, 11688, 27972, 527, 46890, 15592, 0]
ë””ì½”ë”© ê²°ê³¼: <|begin_of_text|>Large Language Models are transforming AI!

ì›ë³¸ê³¼ ë™ì¼?: False
```

</div>


---
title: "Hugging Face ì™„ì „ ì •ë³µ: AI ëª¨ë¸ì˜ GitHub"
author: iwindfree
pubDatetime: 2025-01-28T09:00:00Z
slug: "llm-using-huggingface"
category: "AI Engineering"
tags: ["ai", "llm", "huggingface"]
description: "Hugging FaceëŠ” AI/ML ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” í”Œë«í¼ìœ¼ë¡œ, ìˆ˜ì‹­ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì œê³µí•©ë‹ˆë‹¤. \"AI ëª¨ë¸ì˜ GitHub\"ë¼ê³  ë¶ˆë¦¬ë©°, ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ê³µìœ í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
---

## ê°œìš”

**Hugging Face**ëŠ” AI/ML ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” í”Œë«í¼ìœ¼ë¡œ, ìˆ˜ì‹­ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì œê³µí•©ë‹ˆë‹¤. "AI ëª¨ë¸ì˜ GitHub"ë¼ê³  ë¶ˆë¦¬ë©°, ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ê³µìœ í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í•™ìŠµ ëª©í‘œ

- âœ… Hugging Face ìƒíƒœê³„ ì´í•´ (Hub, Transformers, Datasets)
- âœ… Pipeline APIë¡œ ë¹ ë¥´ê²Œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°
- âœ… ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ ì‹¤ìŠµ (í…ìŠ¤íŠ¸ ìƒì„±, ë¶„ë¥˜, ì„ë² ë”©)
- âœ… ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ í™œìš©
- âœ… ëª¨ë¸ ê²€ìƒ‰ ë° ë‹¤ìš´ë¡œë“œ ë°©ë²•

## ì™œ Hugging Faceì¸ê°€?

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ë°©ëŒ€í•œ ëª¨ë¸ ì €ì¥ì†Œ** | 50ë§Œ+ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ |
| **ì‰¬ìš´ ì‚¬ìš©ë²•** | 3ì¤„ ì½”ë“œë¡œ SOTA ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ |
| **ì»¤ë®¤ë‹ˆí‹°** | í™œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹° |
| **ë¬´ë£Œ** | ëŒ€ë¶€ë¶„ ëª¨ë¸ ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥ |
| **ë‹¤ì–‘í•œ íƒœìŠ¤í¬** | NLP, Vision, Audio, Multimodal ì§€ì› |

---

## 1. Hugging Face í•µì‹¬ ê°œë…

### 1.1 Hugging Face Hub

**Hub**ëŠ” ëª¨ë¸, ë°ì´í„°ì…‹, Spaces(ë°ëª¨ ì•±)ë¥¼ í˜¸ìŠ¤íŒ…í•˜ëŠ” ì¤‘ì•™ ì €ì¥ì†Œì…ë‹ˆë‹¤.

- ğŸ”— **URL**: https://huggingface.co
- ğŸ“¦ **ëª¨ë¸**: GPT, BERT, LLaMA, Stable Diffusion ë“±
- ğŸ“Š **ë°ì´í„°ì…‹**: GLUE, SQuAD, ImageNet ë“±
- ğŸš€ **Spaces**: Gradio/Streamlit ê¸°ë°˜ ë°ëª¨ ì•±

### 1.2 Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬

**Transformers**ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‰½ê²Œ ë¡œë“œí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
classifier("I love Hugging Face!")
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

### 1.3 ì£¼ìš” êµ¬ì„± ìš”ì†Œ

| êµ¬ì„± ìš”ì†Œ | ì—­í•  |
|----------|------|
| **Model** | ì‚¬ì „ í•™ìŠµëœ ì‹ ê²½ë§ (weights) |
| **Tokenizer** | í…ìŠ¤íŠ¸ â†” í† í° ë³€í™˜ |
| **Pipeline** | ì „ì²˜ë¦¬-ì¶”ë¡ -í›„ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— |
| **Trainer** | ëª¨ë¸ í•™ìŠµ/íŒŒì¸íŠœë‹ ë„êµ¬ |
| **Dataset** | ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ |

---

## 2. í™˜ê²½ ì„¤ì •

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤.


```python
# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ)
# !pip install transformers datasets pillow requests
```



```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from transformers import pipeline, AutoModel, AutoTokenizer
from PIL import Image
import requests
from io import BytesIO

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!")
```



<div class="nb-output">

```text
âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!
```

</div>


---

## 3. Pipeline API: ê°€ì¥ ì‰¬ìš´ ì‹œì‘ ë°©ë²•

**Pipeline**ì€ ì „ì²˜ë¦¬, ëª¨ë¸ ì¶”ë¡ , í›„ì²˜ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê³ ìˆ˜ì¤€ APIì…ë‹ˆë‹¤.

### Pipeline ì‚¬ìš© íë¦„

```
ì…ë ¥ í…ìŠ¤íŠ¸ â†’ Tokenizer â†’ Model â†’ Post-processing â†’ ê²°ê³¼
```

ëª¨ë“  ê³¼ì •ì´ ìë™í™”ë˜ì–´ **3ì¤„ ì½”ë“œ**ë¡œ SOTA ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

### 3.1 ê°ì • ë¶„ì„ (Sentiment Analysis)

í…ìŠ¤íŠ¸ì˜ ê°ì •(ê¸ì •/ë¶€ì •)ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

#### Pipelineì´ë€?

**Pipeline**ì€ ë³µì¡í•œ ML ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” ê³ ìˆ˜ì¤€ APIì…ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ 3ë‹¨ê³„ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤:

```
1. ì „ì²˜ë¦¬ (Preprocessing)
   â””â”€ Tokenizer: í…ìŠ¤íŠ¸ â†’ í† í° IDë¡œ ë³€í™˜
   â””â”€ ë°°ì¹˜ ì²˜ë¦¬, íŒ¨ë”©, attention mask ìƒì„±

2. ëª¨ë¸ ì¶”ë¡  (Model Inference)
   â””â”€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— í† í° ì…ë ¥
   â””â”€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ logits(ì ìˆ˜) ê³„ì‚°

3. í›„ì²˜ë¦¬ (Post-processing)
   â””â”€ Softmaxë¡œ í™•ë¥  ë³€í™˜
   â””â”€ ê²°ê³¼ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
```

#### Pipelineì˜ ì¥ì 

âœ… **ê°„í¸í•¨**: 3ì¤„ ì½”ë“œë¡œ SOTA ëª¨ë¸ ì‚¬ìš©  
âœ… **ìë™í™”**: ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìë™ ì²˜ë¦¬  
âœ… **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì‰½ê²Œ êµì²´ ê°€ëŠ¥  
âœ… **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ì…ë ¥ ë™ì‹œ ì²˜ë¦¬ ì§€ì›

#### Pipeline ìƒì„± ì˜µì…˜

`pipeline()` í•¨ìˆ˜ëŠ” ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ì˜ˆì‹œ |
|---------|------|------|
| `task` | ìˆ˜í–‰í•  íƒœìŠ¤í¬ (í•„ìˆ˜) | `"sentiment-analysis"` |
| `model` | ì‚¬ìš©í•  ëª¨ë¸ (ì„ íƒ) | `"distilbert-base-uncased"` |
| `tokenizer` | ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € | `AutoTokenizer.from_pretrained(...)` |
| `device` | ì‹¤í–‰ ë””ë°”ì´ìŠ¤ | `0` (GPU), `-1` (CPU) |
| `batch_size` | ë°°ì¹˜ í¬ê¸° | `8`, `16` |

**ì˜ˆì‹œ:**

```python
# ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (ìë™ ì„ íƒ)
pipeline("sentiment-analysis")

# íŠ¹ì • ëª¨ë¸ ì§€ì •
pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# GPU ì‚¬ìš©
pipeline("sentiment-analysis", device=0)
```


```python
# ============================================
# 1ë‹¨ê³„: Pipeline ìƒì„±
# ============================================

# task="sentiment-analysis"ë¥¼ ì§€ì •í•˜ë©´ ìë™ìœ¼ë¡œ:
# - ì í•©í•œ ëª¨ë¸ ì„ íƒ (ê¸°ë³¸: distilbert-base-uncased-finetuned-sst-2-english)
# - í•´ë‹¹ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
# - ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬ì„±
sentiment_analyzer = pipeline("sentiment-analysis")

print(f"ğŸ“¦ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {sentiment_analyzer.model.name_or_path}")
print(f"ğŸ”§ í† í¬ë‚˜ì´ì €: {sentiment_analyzer.tokenizer.__class__.__name__}\n")

# ============================================
# 2ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ì¤€ë¹„
# ============================================

texts = [
    "I absolutely love this product! It's amazing!",  # ê°•í•œ ê¸ì •
    "This is the worst experience ever.",             # ê°•í•œ ë¶€ì •
    "It's okay, nothing special."                      # ì¤‘ë¦½ì 
]

# ============================================
# 3ë‹¨ê³„: ê°ì • ë¶„ì„ ì‹¤í–‰
# ============================================

# Pipelineì— í…ìŠ¤íŠ¸(ë˜ëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸)ë¥¼ ì „ë‹¬í•˜ë©´:
# - ìë™ìœ¼ë¡œ í† í°í™”
# - ëª¨ë¸ ì¶”ë¡ 
# - ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
sentiment_results = sentiment_analyzer(texts)

# ë‚´ë¶€ì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ì¼:
# 1. Tokenizer: "I love this" â†’ [101, 1045, 2293, 2023, 102]
# 2. Model: [í† í° IDs] â†’ logits [-2.5, 4.8] (NEGATIVE, POSITIVE ì ìˆ˜)
# 3. Softmax: logits â†’ í™•ë¥  [0.0002, 0.9998]
# 4. Argmax: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë ˆì´ë¸” ì„ íƒ â†’ "POSITIVE"

# ============================================
# 4ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥
# ============================================

print("=" * 60)
print("ê°ì • ë¶„ì„ ê²°ê³¼")
print("=" * 60)

for text, sentiment_result in zip(texts, sentiment_results):
    label = sentiment_result['label']        # ì˜ˆì¸¡ëœ ë ˆì´ë¸” (POSITIVE/NEGATIVE)
    score = sentiment_result['score']        # í•´ë‹¹ ë ˆì´ë¸”ì˜ í™•ë¥  (0~1)
    
    # ì´ëª¨ì§€ ì¶”ê°€
    emoji = "ğŸ˜Š" if label == "POSITIVE" else "ğŸ˜"
    
    print(f"\n{emoji} í…ìŠ¤íŠ¸: \"{text}\"")
    print(f"   ê°ì •: {label}")
    print(f"   í™•ë¥ : {score:.4f} ({score*100:.2f}%)")

print("\n" + "=" * 60)
```



<div class="nb-output">

```text
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[3], line 9
      1 # ============================================
      2 # 1ë‹¨ê³„: Pipeline ìƒì„±
      3 # ============================================
   (...)      7 # - í•´ë‹¹ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
      8 # - ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬ì„±
----> 9 sentiment_analyzer = pipeline("sentiment-analysis1")
     11 print(f"ğŸ“¦ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {sentiment_analyzer.model.name_or_path}")
     12 print(f"ğŸ”§ í† í¬ë‚˜ì´ì €: {sentiment_analyzer.tokenizer.__class__.__name__}\n")

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/transformers/pipelines/__init__.py:965, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    958         pipeline_class = get_class_from_dynamic_module(
    959             class_ref,
    960             model,
    961             code_revision=code_revision,
    962             **hub_kwargs,
    963         )
    964 else:
--> 965     normalized_task, targeted_task, task_options = check_task(task)
    966     if pipeline_class is None:
    967         pipeline_class = targeted_task["impl"]

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/transformers/pipelines/__init__.py:536, in check_task(task)
    490 def check_task(task: str) -> tuple[str, dict, Any]:
    491     """
    492     Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and
    493     default models if they exist.
   (...)    534 
    535     """
--> 536     return PIPELINE_REGISTRY.check_task(task)

File ~/workspace/ws.study/ai-engineering/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1549, in PipelineRegistry.check_task(self, task)
   1546         return task, targeted_task, (tokens[1], tokens[3])
   1547     raise KeyError(f"Invalid translation task {task}, use 'translation_XX_to_YY' format")
-> 1549 raise KeyError(
   1550     f"Unknown task {task}, available tasks are {self.get_supported_tasks() + ['translation_XX_to_YY']}"
   1551 )

KeyError: "Unknown task sentiment-analysis1, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']"
```

</div>


#### ê²°ê³¼ êµ¬ì¡° ì´í•´í•˜ê¸°

Pipelineì´ ë°˜í™˜í•˜ëŠ” ê²°ê³¼ëŠ” **ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬**ì…ë‹ˆë‹¤:

```python
[
    {'label': 'POSITIVE', 'score': 0.9998},
    {'label': 'NEGATIVE', 'score': 0.9995}
]
```

**ê° ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°:**

- `label`: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ë ˆì´ë¸” (ë¬¸ìì—´)
  - ê°ì • ë¶„ì„ì˜ ê²½ìš°: `"POSITIVE"` ë˜ëŠ” `"NEGATIVE"`
  - ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ ë ˆì´ë¸” ì‚¬ìš© ê°€ëŠ¥
  
- `score`: í•´ë‹¹ ë ˆì´ë¸”ì˜ í™•ë¥  (float, 0~1)
  - ëª¨ë¸ì´ í•´ë‹¹ ë ˆì´ë¸”ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€
  - 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ í™•ì‹ ë„
  - Softmax í•¨ìˆ˜ë¡œ ê³„ì‚°ë¨

**ì£¼ì˜ì‚¬í•­:**

âš ï¸ `score`ëŠ” í•­ìƒ **ì„ íƒëœ ë ˆì´ë¸”**ì˜ í™•ë¥ ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë ˆì´ë¸”ì˜ í™•ë¥ ì€ `1 - score`ê°€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë‹¤ì¤‘ í´ë˜ìŠ¤ì¸ ê²½ìš°).

#### ê³ ê¸‰ ì˜µì…˜: ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

ê¸°ë³¸ ëª¨ë¸ ëŒ€ì‹  ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# 5ì  ì²™ë„ ê°ì • ë¶„ì„ ëª¨ë¸ ì‚¬ìš© (1~5 stars)
# ì´ ëª¨ë¸ì€ POSITIVE/NEGATIVE ëŒ€ì‹  1~5ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤

star_classifier = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment"
)

# ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´ ë“±)
multilingual_texts = [
    "This restaurant is amazing!",
    "ì´ ì œí’ˆì€ ì •ë§ í›Œë¥­í•´ìš”!",
    "æœ€é«˜ã®ã‚µãƒ¼ãƒ“ã‚¹ã§ã™!"
]

print("5ì  ì²™ë„ ê°ì • ë¶„ì„ (ë‹¤êµ­ì–´ ì§€ì›):\n")
for text in multilingual_texts:
    # Pipeline í˜¸ì¶œ: ë‹¨ì¼ í…ìŠ¤íŠ¸ â†’ [{'label': '...', 'score': ...}]
    # [0]ìœ¼ë¡œ ì²« ë²ˆì§¸(ê·¸ë¦¬ê³  ìœ ì¼í•œ) ê²°ê³¼ ì¶”ì¶œ
    star_result = star_classifier(text)[0]
    
    # labelì€ "5 stars" ê°™ì€ ë¬¸ìì—´
    # [0]ìœ¼ë¡œ ì²« ë²ˆì§¸ ë¬¸ì('5')ë¥¼ ì¶”ì¶œí•˜ì—¬ ë³„ ê°œìˆ˜ ê²°ì •
    stars = "â­" * int(star_result['label'][0])
    
    print(f"í…ìŠ¤íŠ¸: {text}")
    print(f"í‰ê°€: {star_result['label']} {stars}")
    print(f"í™•ë¥ : {star_result['score']:.4f}\n")
```



<div class="nb-output">

```text
Device set to use mps:0
5ì  ì²™ë„ ê°ì • ë¶„ì„ (ë‹¤êµ­ì–´ ì§€ì›):

í…ìŠ¤íŠ¸: This restaurant is amazing!
í‰ê°€: 5 stars â­â­â­â­â­
í™•ë¥ : 0.8884

í…ìŠ¤íŠ¸: ì´ ì œí’ˆì€ ì •ë§ í›Œë¥­í•´ìš”!
í‰ê°€: 5 stars â­â­â­â­â­
í™•ë¥ : 0.7375

í…ìŠ¤íŠ¸: æœ€é«˜ã®ã‚µãƒ¼ãƒ“ã‚¹ã§ã™!
í‰ê°€: 5 stars â­â­â­â­â­
í™•ë¥ : 0.9166
```

</div>


### 3.2 í…ìŠ¤íŠ¸ ìƒì„± (Text Generation)

ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.


```python
# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ (GPT-2 ëª¨ë¸ ì‚¬ìš©)
generator = pipeline("text-generation", model="gpt2")

# í”„ë¡¬í”„íŠ¸
prompt = "Artificial intelligence will"

# í…ìŠ¤íŠ¸ ìƒì„±
generated_texts = generator(
    prompt,
    max_length=50,      # ìµœëŒ€ í† í° ìˆ˜
    num_return_sequences=2,  # ìƒì„±í•  ë¬¸ì¥ ê°œìˆ˜
    temperature=0.8     # ì°½ì˜ì„± ì¡°ì ˆ (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
)

# ê²°ê³¼ ì¶œë ¥
print(f"í”„ë¡¬í”„íŠ¸: {prompt}\n")
for i, gen_result in enumerate(generated_texts, 1):
    print(f"ìƒì„± {i}: {gen_result['generated_text']}\n")
```



<div class="nb-output">

```text
model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]
generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]
tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]
vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]
Device set to use mps:0
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
í”„ë¡¬í”„íŠ¸: Artificial intelligence will

ìƒì„± 1: Artificial intelligence will be used to make robots smarter. And we will be able to create intelligent machines capable of learning from the experiences of human human beings.

AI will come to society in the form of machines that will be able to do things that humans cannot. In this way, AI will become a part of what we call the "human social reality."

And this social reality is the social reality that allows us to become familiar with what we can do with AI. The social reality that allows us to be able to be more creative and more open to new ideas.

This social reality is what makes you more compassionate, is what makes us more free and more creative.

Human beings are not perfect people without a great deal of human experience.

What we are doing now is creating an AI that will be able to think and react better with more human wisdom and compassion.

But there is one problem with this system. It only has one way of thinking, and that is the artificial intelligence of tomorrow that is already in the work of AI.

The biggest problem with this system is that it is already making mistakesâ€”because it has no idea how to make decisionsâ€”so it actually has no way of knowing what kind of decisions are right for it

ìƒì„± 2: Artificial intelligence will be crucial for all industries. So should our workflows on a daily basis.

The fact that most of the world's firms are already on board with automated systems to design and deploy systems that are completely automated in some cases just indicates their interest in automated systems.

But, as I stated, human labor is another big issue. It should be part of this discussion.

The next question is, "How can you support your employees to make that leap?"

When I spoke at the SITIAC conference last month, I talked about how software-based systems like AI and machine learning can provide great value for the future for companies.

And, again, we saw this.

Companies have been working hard on AI, as long as systems like Google and Facebook are on board.

Companies in the software field have been working hard on AI through their partnerships with AI partners like Google, Facebook, IBM in the cloud, AI companies within the digital services industry, and even Microsoft.

And I don't think AI will ever be the thing we want to be.

We need real progress.

But, even if we could, I believe the next time you see an industry that is getting ready to be
```

</div>


### 3.3 ì§ˆë¬¸ ë‹µë³€ (Question Answering)

ë¬¸ë§¥(context)ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.


```python
# ì§ˆë¬¸ ë‹µë³€ íŒŒì´í”„ë¼ì¸
qa_pipeline = pipeline("question-answering")

# ë¬¸ë§¥ê³¼ ì§ˆë¬¸
context = """
Hugging Face is a company that provides tools and infrastructure for machine learning.
It was founded in 2016 and is based in New York City. The company is known for its
Transformers library, which allows developers to easily use pre-trained models.
"""

questions = [
    "When was Hugging Face founded?",
    "Where is Hugging Face based?",
    "What is Hugging Face known for?"
]

# ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¶”ì¶œ
for question in questions:
    qa_result = qa_pipeline(question=question, context=context)
    print(f"ì§ˆë¬¸: {question}")
    print(f"ë‹µë³€: {qa_result['answer']} (í™•ë¥ : {qa_result['score']:.4f})\n")
```


### 3.4 ë²ˆì—­ (Translation)

ë‹¤ì–‘í•œ ì–¸ì–´ ê°„ ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


```python
# ì˜ì–´ â†’ í”„ë‘ìŠ¤ì–´ ë²ˆì—­
translator = pipeline("translation_en_to_fr")

text = "Hugging Face is an amazing platform for AI developers."
translation = translator(text)

print(f"ì›ë¬¸ (ì˜ì–´): {text}")
print(f"ë²ˆì—­ (í”„ë‘ìŠ¤ì–´): {translation[0]['translation_text']}")
```


### 3.5 ìš”ì•½ (Summarization)

ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§§ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.


```python
# ìš”ì•½ íŒŒì´í”„ë¼ì¸
summarizer = pipeline("summarization")

# ê¸´ í…ìŠ¤íŠ¸
article = """
The transformer architecture has revolutionized natural language processing since its 
introduction in 2017. Unlike previous recurrent neural networks, transformers process 
entire sequences in parallel using self-attention mechanisms. This allows them to 
capture long-range dependencies more effectively. Models like BERT, GPT, and T5 are 
all based on the transformer architecture. These models have achieved state-of-the-art 
results on numerous NLP benchmarks and are now widely used in production systems.
"""

# ìš”ì•½ ìƒì„±
summary = summarizer(article, max_length=50, min_length=20)

print("ì›ë¬¸:")
print(article.strip())
print("\nìš”ì•½:")
print(summary[0]['summary_text'])
```


### 3.6 ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification)

Hugging FaceëŠ” NLPë¿ë§Œ ì•„ë‹ˆë¼ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ë„ ì œê³µí•©ë‹ˆë‹¤.


```python
# ì´ë¯¸ì§€ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸
image_classifier = pipeline("image-classification")

# ìƒ˜í”Œ ì´ë¯¸ì§€ URL (ê³ ì–‘ì´ ì‚¬ì§„)
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
response = requests.get(url)
image = Image.open(BytesIO(response.content))

# ì´ë¯¸ì§€ ë¶„ë¥˜
image_predictions = image_classifier(image)

# ìƒìœ„ 3ê°œ ê²°ê³¼ ì¶œë ¥
print("ì´ë¯¸ì§€ ë¶„ë¥˜ ê²°ê³¼:")
for pred in image_predictions[:3]:
    print(f"- {pred['label']}: {pred['score']:.4f}")

# ì´ë¯¸ì§€ í‘œì‹œ
display(image)
```


---

## 4. ê³ ê¸‰ ì‚¬ìš©ë²•: Tokenizerì™€ Model ì§ì ‘ ì‚¬ìš©

Pipelineì€ í¸ë¦¬í•˜ì§€ë§Œ, ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•  ë•ŒëŠ” **Tokenizer**ì™€ **Model**ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.

### 4.1 Tokenizer ì´í•´í•˜ê¸°

TokenizerëŠ” í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì(í† í° ID)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.


```python
# BERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hugging Face is awesome!"

# í† í°í™”
tokens = tokenizer.tokenize(text)
print(f"í† í°: {tokens}")

# í† í° IDë¡œ ë³€í™˜
token_ids = tokenizer.encode(text)
print(f"í† í° ID: {token_ids}")

# IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
decoded = tokenizer.decode(token_ids)
print(f"ë””ì½”ë”©: {decoded}")
```


### 4.2 ì„ë² ë”© ìƒì„± (Sentence Embeddings)

í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚°, ê²€ìƒ‰ ë“±ì— í™œìš©í•©ë‹ˆë‹¤.


```python
import torch
from transformers import AutoModel, AutoTokenizer

# Sentence-BERT ëª¨ë¸ ë¡œë“œ
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text):
    # í† í°í™”
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Mean pooling
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings

# ì˜ˆì œ ë¬¸ì¥ë“¤
sentences = [
    "I love machine learning",
    "I enjoy artificial intelligence",
    "The weather is nice today"
]

# ì„ë² ë”© ìƒì„±
embeddings = [get_embedding(sent) for sent in sentences]

# ì„ë² ë”© í¬ê¸° í™•ì¸
print(f"ì„ë² ë”© ì°¨ì›: {embeddings[0].shape}")

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
from torch.nn.functional import cosine_similarity

sim_01 = cosine_similarity(embeddings[0], embeddings[1]).item()
sim_02 = cosine_similarity(embeddings[0], embeddings[2]).item()

print(f"\nìœ ì‚¬ë„:")
print(f"'{sentences[0]}' vs '{sentences[1]}': {sim_01:.4f}")
print(f"'{sentences[0]}' vs '{sentences[2]}': {sim_02:.4f}")
print(f"\nğŸ’¡ ì²« ë‘ ë¬¸ì¥(ML ê´€ë ¨)ì´ ë” ìœ ì‚¬í•©ë‹ˆë‹¤!")
```


---

## 5. ëª¨ë¸ íƒìƒ‰ ë° ì„ íƒ

### 5.1 Hubì—ì„œ ëª¨ë¸ ê²€ìƒ‰

Hugging Face Hubì—ì„œëŠ” íƒœìŠ¤í¬, ì–¸ì–´, ë¼ì´ì„¼ìŠ¤ ë“±ìœ¼ë¡œ ëª¨ë¸ì„ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ ê²€ìƒ‰:**
- ğŸ”— https://huggingface.co/models
- ì™¼ìª½ í•„í„°ë¡œ íƒœìŠ¤í¬, ë¼ì´ë¸ŒëŸ¬ë¦¬, ì–¸ì–´ ì„ íƒ
- ì¸ê¸°ìˆœ, ìµœì‹ ìˆœ ì •ë ¬ ê°€ëŠ¥

**Pythonìœ¼ë¡œ ê²€ìƒ‰:**


```python
from huggingface_hub import list_models

# ê°ì • ë¶„ì„ ëª¨ë¸ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ)
models = list_models(
    filter="text-classification",
    sort="downloads",
    limit=5
)

print("ì¸ê¸° ê°ì • ë¶„ì„ ëª¨ë¸ TOP 5:")
for i, model in enumerate(models, 1):
    print(f"{i}. {model.modelId}")
```



<div class="nb-output">

```text
ì¸ê¸° ê°ì • ë¶„ì„ ëª¨ë¸ TOP 5:
1. cross-encoder/ms-marco-MiniLM-L6-v2
2. cardiffnlp/twitter-roberta-base-sentiment-latest
3. facebook/bart-large-mnli
4. distilbert/distilbert-base-uncased-finetuned-sst-2-english
5. BAAI/bge-reranker-v2-m3
```

</div>


### 5.2 ëª¨ë¸ ì •ë³´ í™•ì¸


```python
from huggingface_hub import model_info

# GPT-2 ëª¨ë¸ ì •ë³´ ì¡°íšŒ
info = model_info("gpt2")

print(f"ëª¨ë¸ ID: {info.modelId}")
print(f"íŒŒì´í”„ë¼ì¸ íƒœê·¸: {info.pipeline_tag}")
print(f"ë‹¤ìš´ë¡œë“œ ìˆ˜: {info.downloads:,}")
print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬: {info.library_name}")
print(f"ë¼ì´ì„¼ìŠ¤: {info.cardData.get('license', 'N/A')}")
```



<div class="nb-output">

```text
ëª¨ë¸ ID: openai-community/gpt2
íŒŒì´í”„ë¼ì¸ íƒœê·¸: text-generation
ë‹¤ìš´ë¡œë“œ ìˆ˜: 6,397,931
ë¼ì´ë¸ŒëŸ¬ë¦¬: transformers
ë¼ì´ì„¼ìŠ¤: mit
```

</div>


---

## 6. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹±

### 6.1 ìë™ ìºì‹±

Hugging FaceëŠ” ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ìºì‹œí•©ë‹ˆë‹¤:
- **ìœ„ì¹˜**: `~/.cache/huggingface/hub/`
- ë™ì¼ ëª¨ë¸ ì¬ì‚¬ìš© ì‹œ ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ

### 6.2 ì˜¤í”„ë¼ì¸ ì‚¬ìš©

ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì¸í„°ë„· ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
model_name = "distilbert-base-uncased"

print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

print("âœ… ëª¨ë¸ì´ ë¡œì»¬ ìºì‹œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ìºì‹œ ìœ„ì¹˜: ~/.cache/huggingface/hub/")

# ì´ì œ ì˜¤í”„ë¼ì¸ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥
# tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
# model = AutoModel.from_pretrained(model_name, local_files_only=True)
```


---

## 7. ì§€ì›í•˜ëŠ” ì£¼ìš” íƒœìŠ¤í¬

Hugging FaceëŠ” ë‹¤ì–‘í•œ AI íƒœìŠ¤í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:

### NLP (ìì—°ì–´ ì²˜ë¦¬)

| íƒœìŠ¤í¬ | Pipeline ì´ë¦„ | ì˜ˆì‹œ |
|--------|--------------|------|
| í…ìŠ¤íŠ¸ ë¶„ë¥˜ | `text-classification` | ê°ì • ë¶„ì„, ìŠ¤íŒ¸ ë¶„ë¥˜ |
| í† í° ë¶„ë¥˜ | `token-classification` | ê°œì²´ëª… ì¸ì‹(NER) |
| ì§ˆë¬¸ ë‹µë³€ | `question-answering` | SQuAD ìŠ¤íƒ€ì¼ QA |
| í…ìŠ¤íŠ¸ ìƒì„± | `text-generation` | GPT ìŠ¤íƒ€ì¼ ìƒì„± |
| ìš”ì•½ | `summarization` | ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ |
| ë²ˆì—­ | `translation_XX_to_YY` | ë‹¤êµ­ì–´ ë²ˆì—­ |
| ì±„ìš°ê¸° | `fill-mask` | BERT ìŠ¤íƒ€ì¼ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ |

### Computer Vision

| íƒœìŠ¤í¬ | Pipeline ì´ë¦„ |
|--------|-------------|
| ì´ë¯¸ì§€ ë¶„ë¥˜ | `image-classification` |
| ê°ì²´ íƒì§€ | `object-detection` |
| ì´ë¯¸ì§€ ë¶„í•  | `image-segmentation` |
| ì´ë¯¸ì§€ ìƒì„± | Stable Diffusion |

### Audio

| íƒœìŠ¤í¬ | Pipeline ì´ë¦„ |
|--------|-------------|
| ìŒì„± ì¸ì‹ | `automatic-speech-recognition` |
| ì˜¤ë””ì˜¤ ë¶„ë¥˜ | `audio-classification` |
| Text-to-Speech | TTS ëª¨ë¸ |

---

## 8. ê²°ë¡  ë° ìš”ì•½

### í•µì‹¬ ì •ë¦¬

1. **Hugging FaceëŠ” AI ëª¨ë¸ì˜ GitHub**
   - 50ë§Œ+ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸
   - ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥
   - í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°

2. **Pipelineìœ¼ë¡œ ë¹ ë¥¸ ì‹œì‘**
   - 3ì¤„ ì½”ë“œë¡œ SOTA ëª¨ë¸ ì‚¬ìš©
   - ì „ì²˜ë¦¬-ì¶”ë¡ -í›„ì²˜ë¦¬ ìë™í™”
   - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›

3. **ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•˜ë©´ Tokenizer + Model**
   - ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ê°€ëŠ¥
   - ì„ë² ë”© ì¶”ì¶œ
   - íŒŒì¸íŠœë‹ ì¤€ë¹„

4. **ìë™ ìºì‹±ìœ¼ë¡œ íš¨ìœ¨ì  ì‚¬ìš©**
   - í•œ ë²ˆ ë‹¤ìš´ë¡œë“œí•˜ë©´ ì¬ì‚¬ìš©
   - ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥

### ë‹¤ìŒ í•™ìŠµ ë°©í–¥

- âœ… **íŒŒì¸íŠœë‹**: ìì‹ ì˜ ë°ì´í„°ë¡œ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•
- âœ… **Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬
- âœ… **Trainer API**: í•™ìŠµ ì½”ë“œ ê°„ì†Œí™”
- âœ… **Gradio/Spaces**: ëª¨ë¸ ë°ëª¨ ì•± ë°°í¬
- âœ… **Quantization**: ëª¨ë¸ ê²½ëŸ‰í™”ë¡œ ì¶”ë¡  ì†ë„ í–¥ìƒ

### ìœ ìš©í•œ ë§í¬

- ğŸ“š [ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs)
- ğŸ¤— [Hugging Face Hub](https://huggingface.co)
- ğŸ“– [Transformers Course](https://huggingface.co/course)
- ğŸ’¬ [ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼](https://discuss.huggingface.co)

### ì‹¤ìŠµ ê³¼ì œ

1. Hubì—ì„œ í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ì„ ì°¾ì•„ ì‚¬ìš©í•´ë³´ê¸°
2. ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ ë¹„êµí•´ë³´ê¸°
3. ìì‹ ë§Œì˜ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°í•´ë³´ê¸°


```python
print("ğŸ‰ Hugging Face ê¸°ì´ˆ í•™ìŠµ ì™„ë£Œ!")
print("ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìµœì‹  AI ëª¨ë¸ì„ ììœ ë¡­ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
```

